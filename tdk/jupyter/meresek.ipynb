{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d7a68d-38ca-424b-a5b7-35de83dca102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bz2\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a73ab31-1e7e-4fbb-82d8-bab05111bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"\")  \n",
    "DOT_DIR = BASE_DIR / \"dot_files\"  \n",
    "JSON_DIR = BASE_DIR / \"json-files\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95a34d-bed1-45a1-8b20-715d154b381c",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Analysis Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7715735-f7d4-4609-acd9-9575a2c54bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sample data...\n"
     ]
    }
   ],
   "source": [
    "def analyze_xcfa_file(filepath):\n",
    "    \"\"\"Analyze a single XCFA JSON file and extract metrics.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    analysis = {\n",
    "        'filename': Path(filepath).name,\n",
    "        'total_variables': 0,\n",
    "        'total_locations': 0, \n",
    "        'total_edges': 0,\n",
    "        'main_procedure_edges': 0,\n",
    "        'sequence_label_edges': 0,\n",
    "        'total_sequence_labels': 0,\n",
    "        'procedures_count': len(data.get('procedures', [])),\n",
    "        'file_size_kb': Path(filepath).stat().st_size / 1024\n",
    "    }\n",
    "    \n",
    "    # Analyze procedures\n",
    "    for procedure in data.get('procedures', []):\n",
    "        proc_name = procedure.get('name', 'unnamed')\n",
    "        vars_count = len(procedure.get('vars', []))\n",
    "        locs_count = len(procedure.get('locs', []))\n",
    "        edges_count = len(procedure.get('edges', []))\n",
    "        \n",
    "        analysis['total_variables'] += vars_count\n",
    "        analysis['total_locations'] += locs_count\n",
    "        analysis['total_edges'] += edges_count\n",
    "        \n",
    "        # Count sequence labels in main procedure\n",
    "        if proc_name == 'main':\n",
    "            analysis['main_procedure_edges'] = edges_count\n",
    "            for edge in procedure.get('edges', []):\n",
    "                label = edge.get('label', {})\n",
    "                if label.get('type') == 'hu.bme.mit.theta.xcfa.model.SequenceLabel':\n",
    "                    analysis['sequence_label_edges'] += 1\n",
    "                    analysis['total_sequence_labels'] += len(label.get('labels', []))\n",
    "    \n",
    "    # Calculate averages\n",
    "    if analysis['sequence_label_edges'] > 0:\n",
    "        analysis['avg_labels_per_sequence_edge'] = (\n",
    "            analysis['total_sequence_labels'] / analysis['sequence_label_edges']\n",
    "        )\n",
    "    else:\n",
    "        analysis['avg_labels_per_sequence_edge'] = 0\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "def analyze_file_pairs(btor2_dir, c_dir, max_files=None):\n",
    "    \"\"\"Analyze pairs of BTOR2 and C XCFA files.\"\"\"\n",
    "    btor2_files = list(Path(btor2_dir).glob('*.json'))\n",
    "    c_files = list(Path(c_dir).glob('*.json'))\n",
    "    \n",
    "    if max_files:\n",
    "        btor2_files = btor2_files[:max_files]\n",
    "        c_files = c_files[:max_files]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for btor2_file, c_file in zip(btor2_files, c_files):\n",
    "        btor2_analysis = analyze_xcfa_file(btor2_file)\n",
    "        c_analysis = analyze_xcfa_file(c_file)\n",
    "        \n",
    "        if btor2_analysis and c_analysis:\n",
    "            # Calculate ratios and combined metrics\n",
    "            pair_result = {\n",
    "                'file_id': btor2_analysis['filename'].replace('.json', ''),\n",
    "                'btor2_variables': btor2_analysis['total_variables'],\n",
    "                'c_variables': c_analysis['total_variables'],\n",
    "                'variables_ratio': safe_divide(c_analysis['total_variables'], btor2_analysis['total_variables']),\n",
    "                \n",
    "                'btor2_locations': btor2_analysis['total_locations'],\n",
    "                'c_locations': c_analysis['total_locations'], \n",
    "                'locations_ratio': safe_divide(c_analysis['total_locations'], btor2_analysis['total_locations']),\n",
    "                \n",
    "                'btor2_edges': btor2_analysis['total_edges'],\n",
    "                'c_edges': c_analysis['total_edges'],\n",
    "                'edges_ratio': safe_divide(c_analysis['total_edges'], btor2_analysis['total_edges']),\n",
    "                \n",
    "                'btor2_sequence_edges': btor2_analysis['sequence_label_edges'],\n",
    "                'c_sequence_edges': c_analysis['sequence_label_edges'],\n",
    "                'sequence_edges_ratio': safe_divide(c_analysis['sequence_label_edges'], btor2_analysis['sequence_label_edges']),\n",
    "                \n",
    "                'btor2_avg_labels': btor2_analysis['avg_labels_per_sequence_edge'],\n",
    "                'c_avg_labels': c_analysis['avg_labels_per_sequence_edge'],\n",
    "                'avg_labels_ratio': safe_divide(c_analysis['avg_labels_per_sequence_edge'], btor2_analysis['avg_labels_per_sequence_edge']),\n",
    "                \n",
    "                'btor2_procedures': btor2_analysis['procedures_count'],\n",
    "                'c_procedures': c_analysis['procedures_count'],\n",
    "                'procedures_ratio': safe_divide(c_analysis['procedures_count'], btor2_analysis['procedures_count']),\n",
    "                \n",
    "                'btor2_file_size': btor2_analysis['file_size_kb'],\n",
    "                'c_file_size': c_analysis['file_size_kb'],\n",
    "                'file_size_ratio': safe_divide(c_analysis['file_size_kb'], btor2_analysis['file_size_kb'])\n",
    "            }\n",
    "            results.append(pair_result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def safe_divide(a, b):\n",
    "    \"\"\"Safe division to handle zero values.\"\"\"\n",
    "    return a / b if b != 0 else float('inf')\n",
    "\n",
    "# Example usage with sample data\n",
    "print(\"Loading sample data...\")\n",
    "# In practice, you would point to your actual directories\n",
    "# df = analyze_file_pairs('path/to/btor2/files', 'path/to/c/files', max_files=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d9730b-1f63-46b1-ad65-fcc3142f12ee",
   "metadata": {},
   "source": [
    "## 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a989d31-dbef-48c4-b69c-6e3298d0aa6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m\n\u001b[0;32m     16\u001b[0m     c_locs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(btor2_locs \u001b[38;5;241m*\u001b[39m complexity_multiplier))\n\u001b[0;32m     17\u001b[0m     c_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(btor2_edges \u001b[38;5;241m*\u001b[39m complexity_multiplier))\n\u001b[0;32m     19\u001b[0m     sample_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_id\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtor2_variables\u001b[39m\u001b[38;5;124m'\u001b[39m: btor2_vars,\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_variables\u001b[39m\u001b[38;5;124m'\u001b[39m: c_vars,\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: c_vars \u001b[38;5;241m/\u001b[39m btor2_vars,\n\u001b[0;32m     24\u001b[0m         \n\u001b[0;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtor2_locations\u001b[39m\u001b[38;5;124m'\u001b[39m: btor2_locs,\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_locations\u001b[39m\u001b[38;5;124m'\u001b[39m: c_locs,\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocations_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: c_locs \u001b[38;5;241m/\u001b[39m btor2_locs,\n\u001b[0;32m     28\u001b[0m         \n\u001b[0;32m     29\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtor2_edges\u001b[39m\u001b[38;5;124m'\u001b[39m: btor2_edges,\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_edges\u001b[39m\u001b[38;5;124m'\u001b[39m: c_edges,\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medges_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: c_edges \u001b[38;5;241m/\u001b[39m btor2_edges,\n\u001b[0;32m     32\u001b[0m         \n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtor2_sequence_edges\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, btor2_edges \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_sequence_edges\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, c_edges \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m---> 35\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_edges_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43m(\u001b[49m\u001b[43mc_edges\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbtor2_edges\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     36\u001b[0m         \n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtor2_avg_labels\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m),\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_avg_labels\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m2.0\u001b[39m),\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_labels_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m1.5\u001b[39m),\n\u001b[0;32m     40\u001b[0m         \n\u001b[0;32m     41\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtor2_procedures\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mlognormal(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))),\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_procedures\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mlognormal(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.6\u001b[39m))),\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocedures_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mlognormal(\u001b[38;5;241m1.2\u001b[39m, \u001b[38;5;241m0.4\u001b[39m),\n\u001b[0;32m     44\u001b[0m         \n\u001b[0;32m     45\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbtor2_file_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mlognormal(\u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m0.8\u001b[39m)),\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_file_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mlognormal(\u001b[38;5;241m4.5\u001b[39m, \u001b[38;5;241m0.7\u001b[39m)),\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_size_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mlognormal(\u001b[38;5;241m1.8\u001b[39m, \u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m     48\u001b[0m     })\n\u001b[0;32m     50\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sample_data)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated sample data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file pairs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "\n",
    "sample_data = []\n",
    "for i in range(n_files):\n",
    "    sample_data.append()\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(f\"Generated sample data for {len(df)} file pairs\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6dc87-cf46-4a4e-9461-05be890e9129",
   "metadata": {},
   "source": [
    "## 3. Statistical Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89e730f-a7e6-4efb-9db6-8c9674dce098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "metrics = ['variables', 'locations', 'edges', 'sequence_edges', 'procedures', 'file_size']\n",
    "summary_data = []\n",
    "\n",
    "for metric in metrics:\n",
    "    btor2_mean = df[f'btor2_{metric}'].mean()\n",
    "    c_mean = df[f'c_{metric}'].mean()\n",
    "    ratio_mean = df[f'{metric}_ratio'].mean()\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Metric': metric.title(),\n",
    "        'BTOR2_Mean': btor2_mean,\n",
    "        'C_Mean': c_mean,\n",
    "        'C/BTOR2_Ratio': ratio_mean,\n",
    "        'BTOR2_Median': df[f'btor2_{metric}'].median(),\n",
    "        'C_Median': df[f'c_{metric}'].median(),\n",
    "        'Ratio_Median': df[f'{metric}_ratio'].median()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac0296-2b85-4226-914a-0d669b19c7f0",
   "metadata": {},
   "source": [
    "## 4. Distribution Comparison Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454f3ff-58c0-4430-95c3-3bddd9f963c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution comparison plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['variables', 'locations', 'edges', 'sequence_edges', 'procedures', 'file_size']\n",
    "titles = ['Variables', 'Locations', 'Edges', 'Sequence Edges', 'Procedures', 'File Size (KB)']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    # Box plot\n",
    "    data = pd.DataFrame({\n",
    "        'Value': list(df[f'btor2_{metric}']) + list(df[f'c_{metric}']),\n",
    "        'Type': ['BTOR2'] * len(df) + ['C'] * len(df)\n",
    "    })\n",
    "    \n",
    "    sns.boxplot(x='Type', y='Value', data=data, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {title}')\n",
    "    axes[i].set_ylabel(title)\n",
    "    axes[i].set_yscale('log')  # Log scale for better visualization\n",
    "\n",
    "plt.suptitle('Distribution Comparison: BTOR2 vs C XCFA Files', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b337fb14-c060-43dc-83fd-d502b6fa7f7e",
   "metadata": {},
   "source": [
    "## 5. Ratio Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aac289-3e01-41b9-b278-aa2c7dad58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio analysis\n",
    "ratio_metrics = ['variables_ratio', 'locations_ratio', 'edges_ratio', \n",
    "                 'sequence_edges_ratio', 'procedures_ratio', 'file_size_ratio']\n",
    "ratio_titles = ['Variables Ratio', 'Locations Ratio', 'Edges Ratio', \n",
    "                'Sequence Edges Ratio', 'Procedures Ratio', 'File Size Ratio']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(ratio_metrics, ratio_titles)):\n",
    "    # Remove infinite values for plotting\n",
    "    ratios = df[metric].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    \n",
    "    # Histogram of ratios\n",
    "    axes[i].hist(ratios, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i].axvline(x=ratios.median(), color='red', linestyle='--', \n",
    "                   label=f'Median: {ratios.median():.2f}x')\n",
    "    axes[i].axvline(x=1, color='green', linestyle='-', alpha=0.5, label='1x (Equal)')\n",
    "    axes[i].set_xlabel(f'C / BTOR2 Ratio')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_title(f'Distribution of {title}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Complexity Ratio Analysis: C vs BTOR2 XCFA Files', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4df51-ca23-4347-8dd3-ce5784e71d21",
   "metadata": {},
   "source": [
    "## 6. Cumulative Distribution Functions (CDFs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cde69cc-59c4-4e47-945a-bbaf4da8c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDF plots for detailed distribution analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['variables', 'locations', 'edges', 'sequence_edges', 'procedures', 'file_size']\n",
    "titles = ['Variables', 'Locations', 'Edges', 'Sequence Edges', 'Procedures', 'File Size (KB)']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    btor2_vals = sorted(df[f'btor2_{metric}'])\n",
    "    c_vals = sorted(df[f'c_{metric}'])\n",
    "    \n",
    "    # CDF plot\n",
    "    axes[i].plot(btor2_vals, np.linspace(0, 1, len(btor2_vals)), \n",
    "                label='BTOR2', linewidth=2)\n",
    "    axes[i].plot(c_vals, np.linspace(0, 1, len(c_vals)), \n",
    "                label='C', linewidth=2)\n",
    "    axes[i].set_xlabel(title)\n",
    "    axes[i].set_ylabel('CDF')\n",
    "    axes[i].set_title(f'CDF of {title}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].set_xscale('log')\n",
    "\n",
    "plt.suptitle('Cumulative Distribution Functions: BTOR2 vs C XCFA Files', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f81a46-9845-4228-8652-6784d5278fc9",
   "metadata": {},
   "source": [
    "## 7. Scatter Plot Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66418cb9-77ea-4d30-9d33-414a1b7b25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix to show correlations\n",
    "metrics = ['btor2_variables', 'btor2_locations', 'btor2_edges', 'c_variables', 'c_locations', 'c_edges']\n",
    "scatter_df = df[metrics].copy()\n",
    "scatter_df.columns = [col.replace('btor2_', 'B_').replace('c_', 'C_') for col in scatter_df.columns]\n",
    "\n",
    "# Create scatter matrix\n",
    "fig = pd.plotting.scatter_matrix(scatter_df, figsize=(16, 16), diagonal='hist', alpha=0.7)\n",
    "plt.suptitle('Scatter Matrix: BTOR2 vs C XCFA Metrics', fontsize=20, y=0.92)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa7e83-15ac-4ea8-8012-c95bc8903a02",
   "metadata": {},
   "source": [
    "## 8. Complexity Ratio Heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1f00c-e02f-460f-aaf9-710449317ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create complexity ratio heatmap by size categories\n",
    "def create_complexity_heatmap(df):\n",
    "    \"\"\"Create heatmap showing complexity ratios across size categories.\"\"\"\n",
    "    \n",
    "    # Define size categories based on BTOR2 variables\n",
    "    df['size_category'] = pd.cut(df['btor2_variables'], \n",
    "                                bins=[0, 10, 50, 100, np.inf],\n",
    "                                labels=['Small\\n(0-10 vars)', 'Medium\\n(10-50 vars)', \n",
    "                                       'Large\\n(50-100 vars)', 'Very Large\\n(100+ vars)'])\n",
    "    \n",
    "    ratio_metrics = ['variables_ratio', 'locations_ratio', 'edges_ratio', 'file_size_ratio']\n",
    "    metric_names = ['Variables', 'Locations', 'Edges', 'File Size']\n",
    "    \n",
    "    # Calculate mean ratios for each category\n",
    "    heatmap_data = []\n",
    "    categories = df['size_category'].cat.categories\n",
    "    \n",
    "    for category in categories:\n",
    "        category_data = df[df['size_category'] == category]\n",
    "        ratios = []\n",
    "        for metric in ratio_metrics:\n",
    "            # Remove infinite values and calculate mean\n",
    "            valid_ratios = category_data[metric].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "            ratios.append(valid_ratios.mean() if len(valid_ratios) > 0 else np.nan)\n",
    "        heatmap_data.append(ratios)\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    im = ax.imshow(heatmap_data, cmap='RdYlBu_r', aspect='auto', vmin=1, vmax=5)\n",
    "    \n",
    "    # Add labels and annotations\n",
    "    ax.set_xticks(range(len(metric_names)))\n",
    "    ax.set_yticks(range(len(categories)))\n",
    "    ax.set_xticklabels(metric_names)\n",
    "    ax.set_yticklabels(categories)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(categories)):\n",
    "        for j in range(len(metric_names)):\n",
    "            if not np.isnan(heatmap_data[i][j]):\n",
    "                text = ax.text(j, i, f'{heatmap_data[i][j]:.1f}x',\n",
    "                             ha=\"center\", va=\"center\", color=\"black\", \n",
    "                             fontweight='bold', fontsize=12)\n",
    "    \n",
    "    ax.set_title('C-to-BTOR2 Complexity Ratios by Size Category', fontsize=16, pad=20)\n",
    "    plt.colorbar(im, ax=ax, label='Complexity Ratio (C / BTOR2)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_complexity_heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634e1207-86ff-48b2-a1b8-7d81f0ce6251",
   "metadata": {},
   "source": [
    "## 9. Statistical Dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87fa122-f6ce-4aba-b63c-cc3ac547c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive statistical dashboard\n",
    "def create_statistical_dashboard(df):\n",
    "    \"\"\"Create a comprehensive statistical summary dashboard.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Mean comparison bar plot\n",
    "    metrics = ['variables', 'locations', 'edges']\n",
    "    btor2_means = [df[f'btor2_{metric}'].mean() for metric in metrics]\n",
    "    c_means = [df[f'c_{metric}'].mean() for metric in metrics]\n",
    "    \n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax1.bar(x - width/2, btor2_means, width, label='BTOR2', alpha=0.7, color='skyblue')\n",
    "    ax1.bar(x + width/2, c_means, width, label='C', alpha=0.7, color='lightcoral')\n",
    "    ax1.set_ylabel('Mean Value')\n",
    "    ax1.set_title('Mean Complexity Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(['Variables', 'Locations', 'Edges'])\n",
    "    ax1.legend()\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Ratio distribution\n",
    "    ratio_data = []\n",
    "    for metric in metrics:\n",
    "        ratios = df[f'{metric}_ratio'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        ratio_data.append(ratios)\n",
    "    \n",
    "    ax2.boxplot(ratio_data, labels=['Variables', 'Locations', 'Edges'])\n",
    "    ax2.axhline(y=1, color='red', linestyle='--', alpha=0.7, label='Equal Complexity')\n",
    "    ax2.set_ylabel('C / BTOR2 Ratio')\n",
    "    ax2.set_title('Complexity Ratio Distribution')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Correlation heatmap\n",
    "    correlation_metrics = ['btor2_variables', 'btor2_locations', 'btor2_edges',\n",
    "                         'c_variables', 'c_locations', 'c_edges',\n",
    "                         'variables_ratio', 'locations_ratio', 'edges_ratio']\n",
    "    \n",
    "    corr_matrix = df[correlation_metrics].corr()\n",
    "    # Mask upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "                center=0, ax=ax3, square=True, cbar_kws={'shrink': 0.8})\n",
    "    ax3.set_title('Correlation Matrix of XCFA Metrics')\n",
    "    \n",
    "    # 4. Outlier analysis\n",
    "    outlier_counts = {}\n",
    "    for metric in ['variables_ratio', 'locations_ratio', 'edges_ratio']:\n",
    "        ratios = df[metric].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        Q1 = ratios.quantile(0.25)\n",
    "        Q3 = ratios.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = ratios[(ratios < (Q1 - 1.5 * IQR)) | (ratios > (Q3 + 1.5 * IQR))]\n",
    "        outlier_counts[metric.replace('_ratio', '')] = len(outliers)\n",
    "    \n",
    "    ax4.bar(outlier_counts.keys(), outlier_counts.values(), color='orange', alpha=0.7)\n",
    "    ax4.set_ylabel('Number of Outliers')\n",
    "    ax4.set_title('Outlier Count by Metric (IQR Method)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('XCFA Analysis: Comprehensive Statistical Dashboard', fontsize=18, y=0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_statistical_dashboard(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792a5e7-4c87-4b80-b350-b34bcd503974",
   "metadata": {},
   "source": [
    "## 10. Key Insights and Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d9c3c-cf5b-4bec-a1e4-c4a55652924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key insights\n",
    "print(\"KEY INSIGHTS AND CONCLUSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate overall statistics\n",
    "total_files = len(df)\n",
    "high_complexity_files = len(df[df['variables_ratio'] > 3])\n",
    "avg_ratio_vars = df['variables_ratio'].replace([np.inf, -np.inf], np.nan).mean()\n",
    "avg_ratio_locs = df['locations_ratio'].replace([np.inf, -np.inf], np.nan).mean()\n",
    "avg_ratio_edges = df['edges_ratio'].replace([np.inf, -np.inf], np.nan).mean()\n",
    "\n",
    "print(f\"ðŸ“Š Dataset Overview:\")\n",
    "print(f\"   â€¢ Total file pairs analyzed: {total_files}\")\n",
    "print(f\"   â€¢ Files with >3x C complexity: {high_complexity_files} ({high_complexity_files/total_files*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Average Complexity Ratios (C / BTOR2):\")\n",
    "print(f\"   â€¢ Variables: {avg_ratio_vars:.2f}x\")\n",
    "print(f\"   â€¢ Locations: {avg_ratio_locs:.2f}x\") \n",
    "print(f\"   â€¢ Edges: {avg_ratio_edges:.2f}x\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Key Findings:\")\n",
    "print(f\"   â€¢ C-generated XCFAs are typically {avg_ratio_vars:.1f}-{avg_ratio_edges:.1f}x larger than BTOR2 equivalents\")\n",
    "print(f\"   â€¢ This overhead comes from intermediate variables and procedure call handling\")\n",
    "print(f\"   â€¢ BTOR2 versions are more optimized for formal verification\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Recommendations:\")\n",
    "print(f\"   â€¢ Use BTOR2-generated XCFAs for formal verification tasks\")\n",
    "print(f\"   â€¢ Consider C versions only when source-level debugging is needed\")\n",
    "print(f\"   â€¢ The complexity ratio can help estimate verification time differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae372445-5504-4239-81e5-8f66120e0f3c",
   "metadata": {},
   "source": [
    "## 11. Export Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138be10d-28f1-4894-8243-b6281355ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results\n",
    "def export_analysis_results(df, summary_df):\n",
    "    \"\"\"Export analysis results to files.\"\"\"\n",
    "    \n",
    "    # Save detailed data\n",
    "    df.to_csv('xcfa_analysis_detailed.csv', index=False)\n",
    "    summary_df.to_csv('xcfa_analysis_summary.csv', index=False)\n",
    "    \n",
    "    # Save key statistics\n",
    "    with open('xcfa_analysis_insights.txt', 'w') as f:\n",
    "        f.write(\"XCFA Analysis Insights\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(\"Average Complexity Ratios (C / BTOR2):\\n\")\n",
    "        for metric in ['variables', 'locations', 'edges']:\n",
    "            ratio = df[f'{metric}_ratio'].replace([np.inf, -np.inf], np.nan).mean()\n",
    "            f.write(f\"  {metric.title()}: {ratio:.2f}x\\n\")\n",
    "        \n",
    "        f.write(f\"\\nTotal files analyzed: {len(df)}\\n\")\n",
    "        \n",
    "    print(\"âœ… Analysis results exported to:\")\n",
    "    print(\"   - xcfa_analysis_detailed.csv\")\n",
    "    print(\"   - xcfa_analysis_summary.csv\") \n",
    "    print(\"   - xcfa_analysis_insights.txt\")\n",
    "\n",
    "# Export results\n",
    "export_analysis_results(df, summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
