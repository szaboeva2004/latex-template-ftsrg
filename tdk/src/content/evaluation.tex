\chapter{Evaluation}\label{chapter:evaluation}

\section{Experiment Design}\label{sec:experiment_design}

This chapter presents a comprehensive evaluation of the BTOR2CFA transformation approach compared to the traditional translation-based workflow using Btor2C. The evaluation focuses on structural complexity, resource requirements, algorithm performance, and potential performance implications across multiple dimensions.

\subsection{Measurement Procedure} All experiments were conducted on virtual machines equipped with Intel Xeon (Skylake) processors, featuring between 2 and 32 cores operating at 2.2 GHz, and memory configurations ranging from 16 GB to 128 GB of RAM. The operating system used was Linux 6.8.0-60-generic, and all experiments were performed using \textsc{Theta} version 6.11.8. For SMT solving, I employed \textsc{MathSAT} version 5.6.10 \cite{Cimatti2013MathSAT5}.

To ensure reliable and reproducible performance measurements, we utilized the \textsc{RunExec} tool from the \textsc{BenchExec} suite \cite{BLW19}, a state-of-the-art benchmarking framework also used in the \textsc{SV-COMP} competition. Each experiment was executed with a CPU time limit of 900 seconds, a memory limit of 15 GB, and restricted to two CPU cores.

\subsection{Research Questions}

The evaluation aims to answer the following research questions:

\begin{description}
   \item[RQ1:] How does the structural complexity differ between direct BTOR2-to-CFA transformation and the Btor2C translation approach?
   \item[RQ2:] What are the resource implications (memory, processing) of each approach?
   \item[RQ3:] How do the control flow characteristics compare between the two transformation methods?
   \item[RQ4:] Which model checking algorithms perform best for hardware verification, and how does input type (BTOR2 vs C) affect performance?

\end{description}

\subsection{Benchmark Suite and Methodology}

The Hardware Model Checking Competition (HWMCC) provides a comprehensive suite of state-of-the-art benchmark models for evaluating hardware verification tools. In this work, I used a subset of these benchmarks selected according to the following criteria.

First, since my implementation focuses exclusively on safety property verification, only benchmarks containing the \verb|bad| property were considered. Benchmarks featuring liveness-related properties -- such as \verb|justice|, \verb|fair|, \verb|output|, and \verb|constraint| -- were excluded from the evaluation.

Second, several benchmarks in the suite define multiple \verb|bad| properties. To maintain consistency and simplify analysis, I restricted the selection to models containing exactly one safety property.

After applying these filtering criteria, a total of 693 benchmarks remained. The evaluation utilized both the \textsc{BTOR2} and C benchmark subsets provided by HWMCC.

\paragraph{CFA Analysis}
For the complexity analysis, I first examined the Control-Flow Automata (CFAs) generated during the transformation process. It is important to note that the \textsc{c2XCFA} tool includes several optimizations, one of which is Large Block Encoding (LBE)~\cite{lbe}. For the purpose of a fair comparison, this optimization was disabled so that the directly transformed CFA would more closely align with the indirectly transformed one. Benchmarks that resulted in timeouts or memory exhaustion were excluded from the dataset. After filtering, 277 benchmarks remained, which provided a sufficiently representative basis for meaningful analysis.

\paragraph{Performance Analysis}
For the performance evaluation, I compared several verification algorithms supported by \textsc{Theta}. Specifically, I tested Counterexample-Guided Abstraction Refinement (CEGAR)~\cite{cegar} with both explicit-value and predicate abstraction, as well as Bounded Model Checking (BMC)~\cite{bmc}, k-Induction~\cite{kind}, Intrepolation-Based Model Checking (IMC)~\cite{imc}, and IC3~\cite{ic3}.
Each circuit was processed through both transformation workflows:

\begin{itemize}
    \item \textbf{Direct Approach}: BTOR2 $\rightarrow$ CFA (BTOR2XCFA)
    \item \textbf{Indirect Approach}: BTOR2 $\rightarrow$ C $\rightarrow$ CFA (Btor2C + Theta C frontend)
\end{itemize}

Multiple metrics were collected for comparative analysis, including file sizes, structural elements, label complexity, control flow characteristics, and algorithm performance across the different model checking algorithms.

\section{Structural Complexity Analysis}\label{sec:structural}

\subsection{File Size and Storage Requirements}

My first realization was that there were significant differences in file sizes as shown in~\autoref{fig:analysis}

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/complete_cfa_analysis.png}
  \caption{Detailed comparison plots between the resulting BTOR2CFA and c2XCFA CFAs}
  \label{fig:analysis}
\end{figure}

The c2XCFA files generated through Btor2C translation are \textbf{8.87$\times$ larger} on average compared to direct BTOR2 representations, as detailed in \autoref{tab:size_stats}. This substantial size difference has direct implications for storage requirements and I/O operations during verification.

\subsection{Label Complexity Analysis}

The C format exhibits significantly higher label complexity, with \textbf{5.81$\times$ more labels} and longer average label lengths (see \autoref{tab:size_stats}). This increased complexity may impact parsing performance and memory usage during model checking operations.

\section{Control Flow Analysis}\label{sec:control_flow}

\subsection{Main Procedure Characteristics}

The analysis of main procedures reveals substantial differences in control flow complexity as shown in \autoref{tab:main_proc_stats}:

\begin{table}[h]
\centering
\begin{tabular}{l c c c c}
\hline
\textbf{Format} & \textbf{Locations} & \textbf{Edges} & \textbf{Variables} & \textbf{Complexity Score} \\
 & Mean $\pm$ Std & Mean $\pm$ Std & Mean $\pm$ Std & Mean $\pm$ Std \\
\hline
BTOR2 & 2702 $\pm$ 946 & 1200 $\pm$ 416 & 1439 $\pm$ 500 & 1698 $\pm$ 591 \\
C & 9593 $\pm$ 3358 & 4465 $\pm$ 1552 & 3856 $\pm$ 1363 & 5882 $\pm$ 2056 \\
\hline
\end{tabular}
\caption{Main Procedure Statistics comparing structural elements between BTOR2 and C representations. The complexity score is calculated as a weighted combination of locations (30\%), edges (50\%), and variables (20\%) to reflect their relative impact on model checking performance.}
\label{tab:main_proc_stats}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l c c c}
\hline
\textbf{Statistic} & \textbf{Size Ratio} & \textbf{Element Ratio} & \textbf{Label Count Ratio} \\
\hline
Count & 277.00 & 277.00 & 277.00 \\
Mean & 8.87 & 6.04 & 5.81 \\
Std & 0.18 & 0.12 & 0.31 \\
Min & 8.24 & 5.62 & 3.42 \\
25\% & 8.80 & 5.98 & 5.72 \\
50\% & 8.88 & 6.03 & 5.82 \\
75\% & 8.96 & 6.09 & 5.91 \\
Max & 9.55 & 6.77 & 6.64 \\
\hline
\end{tabular}
\caption{File Size Comparison Statistics showing the ratios between C and BTOR2 representations across different complexity metrics.}
\label{tab:size_stats}
\end{table}

\subsection{Complexity Ratios and Performance Implications}

The C representation demonstrates significantly higher complexity across all measured dimensions as shown in \autoref{tab:main_proc_stats}:

\begin{itemize}
    \item \textbf{Locations:} 3.55$\times$ more in C-Bit (9593 vs 2702)
    \item \textbf{Edges:} 3.72$\times$ more in C-Bit (4465 vs 1200)
    \item \textbf{Variables:} 2.68$\times$ more in C-Bit (3856 vs 1439)
    \item \textbf{Overall Complexity:} 3.46$\times$ higher in C-Bit
\end{itemize}

The \textbf{complexity score} used in this analysis is a weighted metric designed to quantify the overall structural complexity of Control-Flow Automata. It combines three key elements with weights reflecting their impact on model checking performance:
\begin{itemize}
    \item \textbf{Edges (50\% weight):} Highest priority due to direct impact on state space explosion
    \item \textbf{Locations (30\% weight):} Medium priority affecting memory usage and abstraction
    \item \textbf{Variables (20\% weight):} Lower priority as modern SMT solvers handle variables efficiently
\end{itemize}

These differences have direct implications for model checking performance:
\begin{itemize}
    \item \textbf{State Space Exploration:} The higher number of edges may exponentially increase the state space exploration complexity
    \item \textbf{Memory Usage:} Increased locations and variables directly impact memory requirements during analysis
    \item \textbf{Analysis Time:} More complex control flow structures may lead to longer verification times
\end{itemize}

\section{Algorithm Performance Analysis}\label{sec:algorithm}

The comprehensive algorithm performance evaluation across 8,316 benchmark runs provides crucial insights into the practical implications of the transformation approach, with detailed results shown in \autoref{fig:performance} and \autoref{tab:algorithm_performance}.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\textwidth]{figures/performance_analysis.png}
  \caption{Detailed comparison plots between BTOR2CFA and c2XCFA tools showing algorithm performance across different metrics.}
  \label{fig:performance}
\end{figure}

\subsection{Overall Performance Landscape}

The analysis reveals a challenging verification landscape with an overall success rate of only \textbf{2.4\%}, dominated by timeouts (76.4\%) and out-of-memory events (8.9\%). This underscores the difficulty of hardware verification and the importance of efficient transformation approaches.

\subsection{BTOR2 vs C Input Performance}

\textbf{BTOR2 Input Type Performance:}
 CEGAR\_EXPL emerged as the top-performing algorithm with 3.0\% success rate and significantly lower average CPU time (26.5s) compared to other approaches. CEGAR\_PRED followed with 3.6\% success rate but higher CPU time (65.8s). The direct BTOR2 transformation consistently enabled faster verification across all algorithms.

\textbf{C Input Type Performance:}
While CEGAR\_PRED achieved the highest success rate (8.8\%) for C inputs, it required substantially more computational resources (190.2s CPU time). CEGAR\_EXPL maintained good performance (2.7\% success) but with increased CPU time (95.4s) compared to BTOR2 inputs.

\subsection{Performance Score Analysis}

The performance score metric (combining success rate and efficiency) clearly favors BTOR2 inputs as demonstrated in \autoref{tab:algorithm_performance}:

\begin{itemize}
    \item \textbf{CEGAR\_EXPL (BTOR2):} 0.449 performance score
    \item \textbf{CEGAR\_EXPL (C):} 0.187 performance score (2.4$\times$ lower)
\end{itemize}

This 2.4$\times$ performance advantage for the direct BTOR2 transformation demonstrates the practical benefits of the structural efficiency observed in previous sections.

\begin{table}[h]
\centering
\begin{tabular}{l c c c c}
\hline
\textbf{Algorithm} & \textbf{Input Type} & \textbf{Success Rate} & \textbf{Avg CPU Time (s)} & \textbf{Performance Score} \\
\hline
CEGAR\_EXPL & BTOR2 & 3.0\% & 26.5 & 0.449 \\
CEGAR\_PRED & BTOR2 & 3.6\% & 65.8 & 0.369 \\
BMC & BTOR2 & 0.6\% & 92.5 & 0.336 \\
K-IND & BTOR2 & 0.6\% & 120.7 & 0.291 \\
IMC & BTOR2 & 0.4\% & 189.9 & 0.184 \\
CEGAR\_EXPL & C & 2.7\% & 95.4 & 0.187 \\
CEGAR\_PRED & C & 8.8\% & 190.2 & 0.070 \\
BMC & C & 3.6\% & 193.8 & 0.064 \\
K-IND & C & 3.0\% & 178.0 & 0.063 \\
IMC & C & 2.9\% & 201.1 & 0.014 \\
IC3 & Both & 0.0\% & 0.0 & 0.000 \\
\hline
\end{tabular}
\caption{Algorithm Performance Comparison showing success rates, CPU times, and performance scores for different model checking algorithms with BTOR2 and C inputs. The performance score combines success rate and efficiency metrics.}
\label{tab:algorithm_performance}
\end{table}

\section{Performance Impact Analysis}\label{sec:performance_impact}

\subsection{Resource Requirements}
Based on the structural and algorithmic analysis, the translation-based approach through Btor2C introduces significant overhead:
\textbf{Memory and Storage Impact}
\begin{itemize}
    \item C-Bit files require \textbf{8.87$\times$ more storage} than direct BTOR2 representations (see \autoref{tab:size_stats})
    \item The increased file sizes may impact I/O performance and memory mapping efficiency
    \item Larger working sets could lead to more cache misses and higher memory bandwidth requirements
\end{itemize}
\textbf{Processing Overhead}

\begin{itemize}
    \item \textbf{5.81$\times$ more labels} require additional parsing and processing time
    \item \textbf{6.03$\times$ more structural elements} increase graph processing complexity
    \item Higher label duplication ratio (0.918 vs 0.886) indicates less efficient representation
\end{itemize}

\subsection{Control Flow Complexity Impact}
The analysis of main procedures in \autoref{tab:main_proc_stats} reveals several performance concerns for the translation-based approach:
\textbf{State Space Exploration}
\begin{itemize}
    \item \textbf{3.72$\times$ more edges} significantly increase branching complexity
    \item Larger state transition systems may lead to exponential growth in state space
    \item Increased path complexity may challenge abstraction refinement algorithms
\end{itemize}

\textbf{Data Flow Analysis}
\begin{itemize}
    \item \textbf{2.68$\times$ more variables} increase the dimensionality of data flow analysis
    \item Larger variable sets require more complex abstract domains and larger SMT queries
    \item Increased memory requirements for storing variable states and relationships
\end{itemize}

\subsection{Algorithmic Efficiency Impact}

The performance results in \autoref{tab:algorithm_performance} demonstrate that:

\begin{itemize}
    \item \textbf{BTOR2 inputs enable faster convergence} across all algorithms, with CEGAR\_EXPL achieving 2.4$\times$ better performance score
    \item \textbf{Reduced structural complexity directly translates to lower CPU times}, particularly for explicit-state methods
    \item \textbf{The direct transformation preserves semantic patterns} that algorithms can exploit more effectively
\end{itemize}

The relationship between structural complexity and algorithm performance is clearly demonstrated by the correlation between the 3.46$\times$ higher complexity score for C representations (from \autoref{tab:main_proc_stats}) and the 2.4$\times$ worse performance score for C inputs (from \autoref{tab:algorithm_performance}).

\section{Threats to Validity}

\subsection{Internal Validity}

\begin{description}
    \item[Benchmark Selection] The 277 circuits from HWMCC represent a comprehensive set of hardware verification benchmarks, but they may not cover all possible hardware verification scenarios. The selection criteria focusing exclusively on safety properties with single bad states, while necessary for controlled comparison, limits the generalizability to circuits with multiple properties or liveness specifications.
    
    \item[Metric Selection] The evaluation primarily focuses on structural complexity metrics such as file sizes, label counts, and control flow elements. While these metrics strongly correlate with verification performance, runtime behavior may exhibit different patterns not captured by static analysis. Additional dynamic metrics could provide complementary insights.
    
    \item[Implementation Maturity] The BTOR2XCFA implementation represents a new frontend component that may lack the sophisticated optimization passes found in mature tools like Btor2C or Theta's C frontend. While the direct transformation demonstrates clear advantages, further tuning and optimization could potentially unlock additional performance gains not reflected in the current evaluation.
    
    \item[Algorithm Configuration] All model checking algorithms were executed using their default parameter configurations without extensive tuning. Fine-tuning algorithm parameters specifically for hardware verification or for particular circuit characteristics might alter the relative performance rankings observed in this study.
\end{description}

\subsection{External Validity}

\begin{description}
    \item[Generalizability] The results obtained in this study are specific to hardware verification using the BTOR2 format. Software verification scenarios or other hardware description languages may exhibit different characteristics and performance patterns. The translation overhead observed in hardware-to-software transformation may not directly apply to native software verification workflows.
    
    \item[Toolchain Dependencies] The findings are inherently tied to the specific versions of Theta (6.11.8), Btor2C, MathSAT (5.6.10), and associated libraries used in the experiments. Different tool versions or alternative SMT solvers might yield different performance characteristics and could affect the relative advantages of the direct transformation approach.
    
    \item[Scale Considerations] The evaluation focuses on medium to large circuits from the HWMCC benchmark suite. Very small circuits might not demonstrate significant differences between transformation approaches, while extremely large circuits might exhibit scalability limitations not captured in the current dataset. The performance advantages observed may vary across different circuit size categories.
    
    \item[Algorithm Selection] The six model checking algorithms tested—CEGAR with explicit and predicate abstraction, BMC, k-Induction, IMC, and IC3—represent common and well-established approaches in the field. However, they do not constitute an exhaustive coverage of all available model checking techniques. Alternative algorithms or hybrid approaches might show different performance patterns with the two transformation methods.
\end{description}