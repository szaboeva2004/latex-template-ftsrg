{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffff6ef",
   "metadata": {},
   "source": [
    "## BMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "\n",
    "class BenchmarkComparator:\n",
    "    def __init__(self):\n",
    "        self.dataframes = {}\n",
    "        self.comparison_dfs = {}\n",
    "        \n",
    "    def parse_xml_file(self, file_path, algorithm_name, input_type):\n",
    "        \"\"\"Parse XML file and extract benchmark results\"\"\"\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        data = []\n",
    "        for run in root.findall('run'):\n",
    "            # Extract benchmark name and properties\n",
    "            run_name = run.get('name', '')\n",
    "            properties = run.get('properties', '')\n",
    "            expected_verdict = run.get('expectedVerdict', '')\n",
    "            \n",
    "            # Extract column values\n",
    "            columns = {}\n",
    "            for col in run.findall('column'):\n",
    "                title = col.get('title')\n",
    "                value = col.get('value')\n",
    "                columns[title] = value\n",
    "            \n",
    "            # Extract file information\n",
    "            files_elem = run.get('files', '[]')\n",
    "            \n",
    "            # Extract algorithm from filename or use provided name\n",
    "            if 'BMC' in algorithm_name.upper():\n",
    "                algo_type = 'BMC'\n",
    "            elif 'CEGAR' in algorithm_name.upper():\n",
    "                if 'predicative' in algorithm_name.lower():\n",
    "                    algo_type = 'CEGAR_Predicative'\n",
    "                elif 'explicit' in algorithm_name.lower():\n",
    "                    algo_type = 'CEGAR_Explicit'\n",
    "                else:\n",
    "                    algo_type = 'CEGAR'\n",
    "            elif 'IMC' in algorithm_name.upper():\n",
    "                algo_type = 'IMC'\n",
    "            elif 'K-INDUCTION' in algorithm_name.upper() or 'K_INDUCTION' in algorithm_name.upper():\n",
    "                algo_type = 'K_Induction'\n",
    "            else:\n",
    "                algo_type = algorithm_name\n",
    "            \n",
    "            row_data = {\n",
    "                'algorithm': algo_type,\n",
    "                'input_type': input_type,\n",
    "                'run_name': run_name,\n",
    "                'properties': properties,\n",
    "                'expected_verdict': expected_verdict,\n",
    "                'status': columns.get('status', ''),\n",
    "                'cputime': float(columns.get('cputime', '0').replace('s', '')) if columns.get('cputime') else 0,\n",
    "                'walltime': float(columns.get('walltime', '0').replace('s', '')) if columns.get('walltime') else 0,\n",
    "                'memory': int(columns.get('memory', '0').replace('B', '')) if columns.get('memory') else 0,\n",
    "                'host': columns.get('host', ''),\n",
    "                'files': files_elem\n",
    "            }\n",
    "            \n",
    "            data.append(row_data)\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def load_all_data(self, result_dir=\"result_xmls\"):\n",
    "        \"\"\"Load all XML files from directory\"\"\"\n",
    "        xml_files = glob.glob(os.path.join(result_dir, \"*.xml\"))\n",
    "        \n",
    "        if not xml_files:\n",
    "            print(f\"No XML files found in {result_dir}\")\n",
    "            return\n",
    "        \n",
    "        for xml_file in xml_files:\n",
    "            filename = os.path.basename(xml_file)\n",
    "            \n",
    "            # Extract algorithm name from filename\n",
    "            algo_name = filename\n",
    "            for pattern in ['BMC', 'CEGAR', 'IMC', 'K-Induction', 'K_Induction']:\n",
    "                if pattern in filename:\n",
    "                    algo_name = pattern\n",
    "                    break\n",
    "            \n",
    "            # Determine input type\n",
    "            if 'btor2' in filename.lower():\n",
    "                input_type = 'btor2'\n",
    "            elif 'c-bit' in filename.lower() or '.c' in filename.lower():\n",
    "                input_type = 'c'\n",
    "            else:\n",
    "                input_type = 'unknown'\n",
    "            \n",
    "            print(f\"Loading {algo_name} ({input_type}) from {filename}\")\n",
    "            \n",
    "            try:\n",
    "                df = self.parse_xml_file(xml_file, algo_name, input_type)\n",
    "                key = f\"{algo_name}_{input_type}\"\n",
    "                self.dataframes[key] = df\n",
    "                print(f\"  Loaded {len(df)} benchmarks\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {filename}: {e}\")\n",
    "    \n",
    "    def load_specific_files(self, file_dict):\n",
    "        \"\"\"Load specific XML files with algorithm names\"\"\"\n",
    "        for algo_name, file_info in file_dict.items():\n",
    "            file_path = file_info['path']\n",
    "            input_type = file_info.get('input_type', 'unknown')\n",
    "            \n",
    "            print(f\"Loading {algo_name} ({input_type}) from {file_path}\")\n",
    "            \n",
    "            try:\n",
    "                df = self.parse_xml_file(file_path, algo_name, input_type)\n",
    "                key = f\"{algo_name}_{input_type}\"\n",
    "                self.dataframes[key] = df\n",
    "                print(f\"  Loaded {len(df)} benchmarks\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error loading {file_path}: {e}\")\n",
    "    \n",
    "    def create_comparison_dataframes(self):\n",
    "        \"\"\"Create comparison dataframes for each algorithm across input types\"\"\"\n",
    "        # Group dataframes by algorithm\n",
    "        algorithm_groups = {}\n",
    "        \n",
    "        for key, df in self.dataframes.items():\n",
    "            # Extract algorithm name (remove input type suffix)\n",
    "            parts = key.split('_')\n",
    "            algo_name = '_'.join(parts[:-1])  # Everything except the last part\n",
    "            \n",
    "            if algo_name not in algorithm_groups:\n",
    "                algorithm_groups[algo_name] = {}\n",
    "            \n",
    "            # Store dataframe with input type\n",
    "            input_type = parts[-1]\n",
    "            algorithm_groups[algo_name][input_type] = df\n",
    "        \n",
    "        # Create comparison for each algorithm\n",
    "        for algo_name, input_dfs in algorithm_groups.items():\n",
    "            if 'btor2' in input_dfs and 'c' in input_dfs:\n",
    "                # Extract benchmark identifiers for matching\n",
    "                def extract_benchmark_id(run_name):\n",
    "                    start = run_name.rfind(\"/\") + 1\n",
    "                    end = run_name.rfind(\".yml\")\n",
    "                    return run_name[start:end] if end != -1 else run_name[start:]\n",
    "                \n",
    "                # Add benchmark IDs\n",
    "                df_btor2 = input_dfs['btor2'].copy()\n",
    "                df_c = input_dfs['c'].copy()\n",
    "                \n",
    "                df_btor2['benchmark_id'] = df_btor2['run_name'].apply(extract_benchmark_id)\n",
    "                df_c['benchmark_id'] = df_c['run_name'].apply(extract_benchmark_id)\n",
    "                \n",
    "                # Merge dataframes on benchmark_id\n",
    "                merged = pd.merge(\n",
    "                    df_btor2, \n",
    "                    df_c, \n",
    "                    on='benchmark_id', \n",
    "                    suffixes=('_btor2', '_c'),\n",
    "                    how='inner'\n",
    "                )\n",
    "                \n",
    "                self.comparison_dfs[algo_name] = merged\n",
    "                print(f\"Created comparison for {algo_name}: {len(merged)} matched benchmarks\")\n",
    "        \n",
    "        return self.comparison_dfs\n",
    "    \n",
    "    def create_algorithm_comparison_plots(self):\n",
    "        \"\"\"Create comparison plots for each algorithm\"\"\"\n",
    "        if not self.comparison_dfs:\n",
    "            self.create_comparison_dataframes()\n",
    "        \n",
    "        # Create individual plots for each algorithm\n",
    "        for algo_name, comp_df in self.comparison_dfs.items():\n",
    "            self._create_single_algorithm_plots(algo_name, comp_df)\n",
    "        \n",
    "        # Create aggregated comparison across all algorithms\n",
    "        self._create_aggregated_comparison()\n",
    "    \n",
    "    def _create_single_algorithm_plots(self, algo_name, comp_df):\n",
    "        \"\"\"Create plots for a single algorithm\"\"\"\n",
    "        # Create directory for algorithm plots\n",
    "        algo_dir = f\"plots_{algo_name}\"\n",
    "        os.makedirs(algo_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Performance Comparison Plot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle(f'Performance Comparison: {algo_name} - BTOR2 vs C Input Types', fontsize=16)\n",
    "        \n",
    "        # CPU Time Comparison\n",
    "        axes[0, 0].scatter(comp_df['cputime_btor2'], \n",
    "                          comp_df['cputime_c'], alpha=0.6)\n",
    "        axes[0, 0].plot([0, comp_df[['cputime_btor2', 'cputime_c']].max().max()], \n",
    "                       [0, comp_df[['cputime_btor2', 'cputime_c']].max().max()], \n",
    "                       'r--', alpha=0.8)\n",
    "        axes[0, 0].set_xlabel('BTOR2 CPU Time (s)')\n",
    "        axes[0, 0].set_ylabel('C CPU Time (s)')\n",
    "        axes[0, 0].set_title('CPU Time Comparison')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Memory Usage Comparison\n",
    "        axes[1, 0].scatter(comp_df['memory_btor2']/1e9, \n",
    "                          comp_df['memory_c']/1e9, alpha=0.6)\n",
    "        max_mem = comp_df[['memory_btor2', 'memory_c']].max().max()/1e9\n",
    "        axes[1, 0].plot([0, max_mem], [0, max_mem], 'r--', alpha=0.8)\n",
    "        axes[1, 0].set_xlabel('BTOR2 Memory (GB)')\n",
    "        axes[1, 0].set_ylabel('C Memory (GB)')\n",
    "        axes[1, 0].set_title('Memory Usage Comparison')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Status Distribution\n",
    "        status_counts = pd.DataFrame({\n",
    "            'BTOR2': comp_df['status_btor2'].value_counts(),\n",
    "            'C': comp_df['status_c'].value_counts()\n",
    "        }).fillna(0)\n",
    "        \n",
    "        status_counts.plot(kind='bar', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Status Distribution Comparison')\n",
    "        axes[1, 1].set_ylabel('Count')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(algo_dir, f'{algo_name}_performance_comparison.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Speedup Analysis\n",
    "        self._create_speedup_analysis(algo_name, comp_df, algo_dir)\n",
    "        \n",
    "        # 3. Status Comparison Matrix\n",
    "        self._create_status_matrix(algo_name, comp_df, algo_dir)\n",
    "    \n",
    "    def _create_speedup_analysis(self, algo_name, comp_df, output_dir):\n",
    "        \"\"\"Create speedup analysis for a single algorithm\"\"\"\n",
    "        # Calculate speedup factors\n",
    "        comp_df['cpu_speedup'] = comp_df['cputime_btor2'] / comp_df['cputime_c']\n",
    "        comp_df['wall_speedup'] = comp_df['walltime_btor2'] / comp_df['walltime_c']\n",
    "        comp_df['memory_ratio'] = comp_df['memory_btor2'] / comp_df['memory_c']\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle(f'Speedup Analysis: {algo_name} - BTOR2 vs C', fontsize=16)\n",
    "        \n",
    "        # CPU Speedup distribution\n",
    "        cpu_data = comp_df['cpu_speedup'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        axes[0].hist(cpu_data, bins=50, alpha=0.7, color='skyblue')\n",
    "        axes[0].axvline(1, color='red', linestyle='--', label='Equal Performance')\n",
    "        axes[0].set_xlabel('CPU Speedup (BTOR2/C)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].set_title('CPU Time Speedup Distribution')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Wall Speedup distribution\n",
    "        wall_data = comp_df['wall_speedup'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        axes[1].hist(wall_data, bins=50, alpha=0.7, color='lightgreen')\n",
    "        axes[1].axvline(1, color='red', linestyle='--', label='Equal Performance')\n",
    "        axes[1].set_xlabel('Wall Speedup (BTOR2/C)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Wall Time Speedup Distribution')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Memory ratio distribution\n",
    "        mem_data = comp_df['memory_ratio'].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        axes[2].hist(mem_data, bins=50, alpha=0.7, color='salmon')\n",
    "        axes[2].axvline(1, color='red', linestyle='--', label='Equal Memory')\n",
    "        axes[2].set_xlabel('Memory Ratio (BTOR2/C)')\n",
    "        axes[2].set_ylabel('Frequency')\n",
    "        axes[2].set_title('Memory Usage Ratio Distribution')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{algo_name}_speedup_analysis.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"\\n{algo_name} Speedup Analysis Summary:\")\n",
    "        print(f\"Average CPU Speedup: {cpu_data.mean():.2f}\")\n",
    "        print(f\"Average Wall Speedup: {wall_data.mean():.2f}\")\n",
    "        print(f\"Average Memory Ratio: {mem_data.mean():.2f}\")\n",
    "    \n",
    "    def _create_status_matrix(self, algo_name, comp_df, output_dir):\n",
    "        \"\"\"Create status comparison matrix for a single algorithm\"\"\"\n",
    "        status_matrix = pd.crosstab(\n",
    "            comp_df['status_btor2'], \n",
    "            comp_df['status_c'],\n",
    "            margins=True\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(status_matrix.iloc[:-1, :-1], annot=True, fmt='d', cmap='YlOrRd')\n",
    "        plt.title(f'Status Transition Matrix: {algo_name} - BTOR2 vs C')\n",
    "        plt.xlabel('C Status')\n",
    "        plt.ylabel('BTOR2 Status')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'{algo_name}_status_matrix.png'), \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    def _create_aggregated_comparison(self):\n",
    "        \"\"\"Create aggregated comparison across all algorithms\"\"\"\n",
    "        if len(self.comparison_dfs) < 2:\n",
    "            print(\"Need at least 2 algorithms for aggregated comparison\")\n",
    "            return\n",
    "        \n",
    "        # Prepare aggregated data\n",
    "        aggregated_data = []\n",
    "        for algo_name, comp_df in self.comparison_dfs.items():\n",
    "            # Calculate average metrics\n",
    "            avg_cpu_btor2 = comp_df['cputime_btor2'].mean()\n",
    "            avg_cpu_c = comp_df['cputime_c'].mean()\n",
    "            avg_mem_btor2 = comp_df['memory_btor2'].mean() / 1e9  # GB\n",
    "            avg_mem_c = comp_df['memory_c'].mean() / 1e9  # GB\n",
    "            \n",
    "            # Count statuses\n",
    "            success_btor2 = len(comp_df[comp_df['status_btor2'] != 'TIMEOUT'])\n",
    "            success_c = len(comp_df[comp_df['status_c'] != 'TIMEOUT'])\n",
    "            total = len(comp_df)\n",
    "            \n",
    "            aggregated_data.append({\n",
    "                'Algorithm': algo_name,\n",
    "                'Avg_CPU_BTOR2': avg_cpu_btor2,\n",
    "                'Avg_CPU_C': avg_cpu_c,\n",
    "                'Avg_Mem_BTOR2': avg_mem_btor2,\n",
    "                'Avg_Mem_C': avg_mem_c,\n",
    "                'Success_BTOR2': success_btor2,\n",
    "                'Success_C': success_c,\n",
    "                'Total': total\n",
    "            })\n",
    "        \n",
    "        agg_df = pd.DataFrame(aggregated_data)\n",
    "        \n",
    "        # Create aggregated plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # CPU Time comparison across algorithms\n",
    "        x = np.arange(len(agg_df))\n",
    "        width = 0.35\n",
    "        axes[0, 0].bar(x - width/2, agg_df['Avg_CPU_BTOR2'], width, label='BTOR2')\n",
    "        axes[0, 0].bar(x + width/2, agg_df['Avg_CPU_C'], width, label='C')\n",
    "        axes[0, 0].set_xlabel('Algorithm')\n",
    "        axes[0, 0].set_ylabel('Average CPU Time (s)')\n",
    "        axes[0, 0].set_title('Average CPU Time Comparison Across Algorithms')\n",
    "        axes[0, 0].set_xticks(x)\n",
    "        axes[0, 0].set_xticklabels(agg_df['Algorithm'], rotation=45)\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Memory comparison across algorithms\n",
    "        axes[0, 1].bar(x - width/2, agg_df['Avg_Mem_BTOR2'], width, label='BTOR2')\n",
    "        axes[0, 1].bar(x + width/2, agg_df['Avg_Mem_C'], width, label='C')\n",
    "        axes[0, 1].set_xlabel('Algorithm')\n",
    "        axes[0, 1].set_ylabel('Average Memory (GB)')\n",
    "        axes[0, 1].set_title('Average Memory Usage Comparison Across Algorithms')\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels(agg_df['Algorithm'], rotation=45)\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Success rate comparison\n",
    "        success_rate_btor2 = agg_df['Success_BTOR2'] / agg_df['Total'] * 100\n",
    "        success_rate_c = agg_df['Success_C'] / agg_df['Total'] * 100\n",
    "        axes[1, 0].bar(x - width/2, success_rate_btor2, width, label='BTOR2')\n",
    "        axes[1, 0].bar(x + width/2, success_rate_c, width, label='C')\n",
    "        axes[1, 0].set_xlabel('Algorithm')\n",
    "        axes[1, 0].set_ylabel('Success Rate (%)')\n",
    "        axes[1, 0].set_title('Success Rate Comparison Across Algorithms')\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(agg_df['Algorithm'], rotation=45)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Speedup factors across algorithms\n",
    "        cpu_speedup = agg_df['Avg_CPU_BTOR2'] / agg_df['Avg_CPU_C']\n",
    "        mem_speedup = agg_df['Avg_Mem_BTOR2'] / agg_df['Avg_Mem_C']\n",
    "        axes[1, 1].plot(agg_df['Algorithm'], cpu_speedup, 'o-', label='CPU Speedup')\n",
    "        axes[1, 1].plot(agg_df['Algorithm'], mem_speedup, 's-', label='Memory Speedup')\n",
    "        axes[1, 1].axhline(1, color='red', linestyle='--', label='Baseline (1.0)')\n",
    "        axes[1, 1].set_xlabel('Algorithm')\n",
    "        axes[1, 1].set_ylabel('Speedup Ratio (BTOR2/C)')\n",
    "        axes[1, 1].set_title('Speedup Factors Across Algorithms')\n",
    "        axes[1, 1].set_xticklabels(agg_df['Algorithm'], rotation=45)\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('aggregated_algorithm_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return agg_df\n",
    "    \n",
    "    def generate_summary_reports(self):\n",
    "        \"\"\"Generate summary reports for each algorithm\"\"\"\n",
    "        if not self.comparison_dfs:\n",
    "            self.create_comparison_dataframes()\n",
    "        \n",
    "        for algo_name, comp_df in self.comparison_dfs.items():\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"SUMMARY REPORT: {algo_name}\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            print(f\"\\nTotal Benchmarks Compared: {len(comp_df)}\")\n",
    "            \n",
    "            # Status summary\n",
    "            print(\"\\nStatus Distribution:\")\n",
    "            print(\"BTOR2:\")\n",
    "            print(comp_df['status_btor2'].value_counts())\n",
    "            print(\"\\nC:\")\n",
    "            print(comp_df['status_c'].value_counts())\n",
    "            \n",
    "            # Performance metrics\n",
    "            print(\"\\nPerformance Metrics (Average):\")\n",
    "            metrics = ['cputime', 'walltime', 'memory']\n",
    "            for metric in metrics:\n",
    "                btor2_avg = comp_df[f'{metric}_btor2'].mean()\n",
    "                c_avg = comp_df[f'{metric}_c'].mean()\n",
    "                ratio = btor2_avg / c_avg if c_avg > 0 else float('inf')\n",
    "                print(f\"{metric.upper()}: BTOR2={btor2_avg:.2f}, C={c_avg:.2f}, Ratio={ratio:.2f}\")\n",
    "            \n",
    "            # Success rate comparison\n",
    "            btor2_success = len(comp_df[comp_df['status_btor2'] != 'TIMEOUT'])\n",
    "            c_success = len(comp_df[comp_df['status_c'] != 'TIMEOUT'])\n",
    "            \n",
    "            print(f\"\\nSuccess Rates:\")\n",
    "            print(f\"BTOR2: {btor2_success/len(comp_df)*100:.1f}%\")\n",
    "            print(f\"C: {c_success/len(comp_df)*100:.1f}%\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize comparator\n",
    "    comparator = BenchmarkComparator()\n",
    "    \n",
    "    # Option 1: Load all XML files from directory\n",
    "    print(\"Loading all XML files from result_xmls directory...\")\n",
    "    comparator.load_all_data(\"result_xmls\")\n",
    "    \n",
    "    # Option 2: Or load specific files with algorithm names\n",
    "    \"\"\"\n",
    "    file_dict = {\n",
    "        'BMC': {\n",
    "            'path': \"result_xmls\\\\theta_algos.2025-10-29_06-24-29.results.btor2-bounded.btor2.xml\",\n",
    "            'input_type': 'btor2'\n",
    "        },\n",
    "        'BMC_C': {\n",
    "            'path': \"result_xmls\\\\theta_algos.2025-10-29_06-24-29.results.c-bit-bounded.c-bit.xml\",\n",
    "            'input_type': 'c'\n",
    "        },\n",
    "        'CEGAR_Explicit': {\n",
    "            'path': \"path_to_cegar_explicit_btor2.xml\",\n",
    "            'input_type': 'btor2'\n",
    "        },\n",
    "        # ... add more algorithms\n",
    "    }\n",
    "    comparator.load_specific_files(file_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate all analyses\n",
    "    if comparator.dataframes:\n",
    "        print(\"\\nCreating comparison dataframes...\")\n",
    "        comparator.create_comparison_dataframes()\n",
    "        \n",
    "        print(\"\\nGenerating plots for each algorithm...\")\n",
    "        comparator.create_algorithm_comparison_plots()\n",
    "        \n",
    "        print(\"\\nGenerating summary reports...\")\n",
    "        comparator.generate_summary_reports()\n",
    "        \n",
    "        print(\"\\nAll analyses completed! Check the generated plot directories.\")\n",
    "    else:\n",
    "        print(\"No data loaded. Please check your XML files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9e08b",
   "metadata": {},
   "source": [
    "## All Algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d0dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class AlgorithmComparator:\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.all_data = pd.DataFrame()\n",
    "        self.algorithm_results = {}\n",
    "        \n",
    "    def detect_algorithm_from_filename(self, filename):\n",
    "        \"\"\"Detect algorithm type from filename patterns\"\"\"\n",
    "        filename_lower = filename.lower()\n",
    "        \n",
    "        if 'c-bit' in filename_lower:\n",
    "            input_type = 'C'\n",
    "        elif 'btor2' in filename_lower:\n",
    "            input_type = 'BTOR2'\n",
    "        else:\n",
    "            input_type = 'UNKNOWN'\n",
    "            \n",
    "        # Detect algorithm\n",
    "        if 'cegar' in filename_lower and 'pred' in filename_lower:\n",
    "            algorithm = 'CEGAR_PRED'\n",
    "        elif 'cegar' in filename_lower and 'expl' in filename_lower:\n",
    "            algorithm = 'CEGAR_EXPL'\n",
    "        elif 'bounded' in filename_lower:\n",
    "            algorithm = 'BMC'\n",
    "        elif 'imc' in filename_lower:\n",
    "            algorithm = 'IMC'\n",
    "        elif 'kind' in filename_lower:\n",
    "            algorithm = 'K-IND'\n",
    "        elif 'ic3' in filename_lower:\n",
    "            algorithm = 'IC3'\n",
    "        else:\n",
    "            algorithm = 'UNKNOWN'\n",
    "            \n",
    "        return algorithm, input_type\n",
    "    \n",
    "    def parse_xml_file(self, file_path, algorithm, input_type):\n",
    "        \"\"\"Parse XML file and extract benchmark results\"\"\"\n",
    "        try:\n",
    "            tree = ET.parse(file_path)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            data = []\n",
    "            for run in root.findall('run'):\n",
    "                # Extract benchmark name and properties\n",
    "                run_name = run.get('name', '')\n",
    "                properties = run.get('properties', '')\n",
    "                expected_verdict = run.get('expectedVerdict', '')\n",
    "                \n",
    "                # Extract column values\n",
    "                columns = {}\n",
    "                for col in run.findall('column'):\n",
    "                    title = col.get('title')\n",
    "                    value = col.get('value')\n",
    "                    columns[title] = value\n",
    "                \n",
    "                # Extract benchmark category\n",
    "                benchmark_category = self.extract_benchmark_category(run_name)\n",
    "                \n",
    "                # FIX: Correct success detection - consider both 'true' and 'false' status as successful if they complete\n",
    "                status = columns.get('status', '')\n",
    "                # Consider runs with status 'true', 'false', or 'correct' as successful completions\n",
    "                is_successful = status in ['true', 'false', 'correct', 'false(unreach-call)', ]\n",
    "                # Consider timeouts and OOM as failures\n",
    "                is_timeout = status == 'timeout' or 'timeout' in status.lower()\n",
    "                is_oom = status == 'out of memory' or 'memory' in status.lower()\n",
    "                \n",
    "                row_data = {\n",
    "                    'algorithm': algorithm,\n",
    "                    'input_type': input_type,\n",
    "                    'file_name': os.path.basename(file_path),\n",
    "                    'run_name': run_name,\n",
    "                    'benchmark_category': benchmark_category,\n",
    "                    'benchmark_name': self.extract_benchmark_name(run_name),\n",
    "                    'properties': properties,\n",
    "                    'expected_verdict': expected_verdict,\n",
    "                    'status': status,\n",
    "                    'cputime': self.safe_float(columns.get('cputime', '0').replace('s', '')),\n",
    "                    'walltime': self.safe_float(columns.get('walltime', '0').replace('s', '')),\n",
    "                    'memory': self.safe_int(columns.get('memory', '0').replace('B', '')),\n",
    "                    'host': columns.get('host', ''),\n",
    "                    'success': 1 if is_successful else 0,\n",
    "                    'timeout': 1 if is_timeout else 0,\n",
    "                    'oom': 1 if is_oom else 0\n",
    "                }\n",
    "                \n",
    "                data.append(row_data)\n",
    "            \n",
    "            return pd.DataFrame(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def safe_float(self, value):\n",
    "        \"\"\"Safely convert to float\"\"\"\n",
    "        try:\n",
    "            return float(value)\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def safe_int(self, value):\n",
    "        \"\"\"Safely convert to int\"\"\"\n",
    "        try:\n",
    "            return int(value)\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def extract_benchmark_category(self, run_name):\n",
    "        \"\"\"Extract benchmark category from run name\"\"\"\n",
    "        categories = ['adding', 'anderson', 'at', 'bakery', 'blocks', 'bridge', 'brp', 'brp2']\n",
    "        for category in categories:\n",
    "            if category in run_name.lower():\n",
    "                return category\n",
    "        return 'other'\n",
    "    \n",
    "    def extract_benchmark_name(self, run_name):\n",
    "        \"\"\"Extract simplified benchmark name\"\"\"\n",
    "        # Extract the core benchmark identifier\n",
    "        start = run_name.rfind(\"/\") + 1\n",
    "        end = run_name.rfind(\".yml\")\n",
    "        return run_name[start:end] if end != -1 else run_name[start:]\n",
    "    \n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load and parse all XML files in the folder\"\"\"\n",
    "        xml_files = list(Path(self.folder_path).glob('*.xml'))\n",
    "        print(f\"Found {len(xml_files)} XML files\")\n",
    "        \n",
    "        all_dfs = []\n",
    "        \n",
    "        for xml_file in xml_files:\n",
    "            algorithm, input_type = self.detect_algorithm_from_filename(xml_file.name)\n",
    "            print(f\"Processing: {xml_file.name} -> {algorithm}/{input_type}\")\n",
    "            \n",
    "            df = self.parse_xml_file(xml_file, algorithm, input_type)\n",
    "            if not df.empty:\n",
    "                all_dfs.append(df)\n",
    "                print(f\"  Loaded {len(df)} runs, {df['success'].sum()} successful\")\n",
    "        \n",
    "        if all_dfs:\n",
    "            self.all_data = pd.concat(all_dfs, ignore_index=True)\n",
    "            print(f\"\\nLoaded {len(self.all_data)} benchmark results\")\n",
    "            print(f\"Algorithms: {self.all_data['algorithm'].unique()}\")\n",
    "            print(f\"Input types: {self.all_data['input_type'].unique()}\")\n",
    "            \n",
    "            # Print success statistics by algorithm and input type\n",
    "            print(\"\\nSuccess counts by algorithm and input type:\")\n",
    "            success_counts = self.all_data.groupby(['algorithm', 'input_type'])['success'].agg(['sum', 'count'])\n",
    "            success_counts['success_rate'] = (success_counts['sum'] / success_counts['count'] * 100).round(1)\n",
    "            print(success_counts)\n",
    "        else:\n",
    "            print(\"No data loaded!\")\n",
    "        \n",
    "        return self.all_data\n",
    "    \n",
    "    def calculate_algorithm_metrics(self):\n",
    "        \"\"\"Calculate performance metrics for each algorithm\"\"\"\n",
    "        if self.all_data.empty:\n",
    "            self.load_all_data()\n",
    "        \n",
    "        metrics = []\n",
    "        \n",
    "        for (algorithm, input_type), group in self.all_data.groupby(['algorithm', 'input_type']):\n",
    "            total_benchmarks = len(group)\n",
    "            successful_count = group['success'].sum()\n",
    "            success_rate = (successful_count / total_benchmarks) * 100 if total_benchmarks > 0 else 0\n",
    "            timeout_rate = group['timeout'].mean() * 100\n",
    "            oom_rate = group['oom'].mean() * 100\n",
    "            \n",
    "            # Average performance metrics (only for successful runs)\n",
    "            successful_runs = group[group['success'] == 1]\n",
    "            avg_cputime = successful_runs['cputime'].mean() if len(successful_runs) > 0 else 0\n",
    "            avg_walltime = successful_runs['walltime'].mean() if len(successful_runs) > 0 else 0\n",
    "            avg_memory = successful_runs['memory'].mean() if len(successful_runs) > 0 else 0\n",
    "            \n",
    "            metrics.append({\n",
    "                'algorithm': algorithm,\n",
    "                'input_type': input_type,\n",
    "                'total_benchmarks': total_benchmarks,\n",
    "                'success_rate': success_rate,\n",
    "                'timeout_rate': timeout_rate,\n",
    "                'oom_rate': oom_rate,\n",
    "                'avg_cputime': avg_cputime,\n",
    "                'avg_walltime': avg_walltime,\n",
    "                'avg_memory_gb': avg_memory / 1e9,\n",
    "                'successful_runs': successful_count\n",
    "            })\n",
    "        \n",
    "        self.algorithm_metrics = pd.DataFrame(metrics)\n",
    "        return self.algorithm_metrics\n",
    "    \n",
    "    def plot_success_rates(self):\n",
    "        \"\"\"Plot success rates for all algorithms\"\"\"\n",
    "        if not hasattr(self, 'algorithm_metrics'):\n",
    "            self.calculate_algorithm_metrics()\n",
    "        \n",
    "        # Set Times New Roman font and larger text sizes\n",
    "        plt.rcParams['font.family'] = 'Times New Roman'\n",
    "        plt.rcParams['font.size'] = 14\n",
    "        plt.rcParams['axes.titlesize'] = 16\n",
    "        plt.rcParams['axes.labelsize'] = 14\n",
    "        plt.rcParams['xtick.labelsize'] = 12\n",
    "        plt.rcParams['ytick.labelsize'] = 12\n",
    "        plt.rcParams['legend.fontsize'] = 12\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "        fig.suptitle('Algorithm Performance Comparison', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        # Success rates by algorithm and input type\n",
    "        pivot_success = self.algorithm_metrics.pivot(index='algorithm', columns='input_type', values='success_rate')\n",
    "        pivot_success.plot(kind='bar', ax=axes[0, 0], color=['skyblue', 'lightcoral'])\n",
    "        axes[0, 0].set_title('Success Rate by Algorithm and Input Type', fontweight='bold', fontsize=16)\n",
    "        axes[0, 0].set_ylabel('Success Rate (%)', fontsize=14)\n",
    "        axes[0, 0].set_xlabel('Algorithm', fontsize=14)\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].legend(title='Input Type', title_fontsize=12)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Timeout rates\n",
    "        pivot_timeout = self.algorithm_metrics.pivot(index='algorithm', columns='input_type', values='timeout_rate')\n",
    "        pivot_timeout.plot(kind='bar', ax=axes[0, 1], color=['skyblue', 'lightcoral'])\n",
    "        axes[0, 1].set_title('Timeout Rate by Algorithm and Input Type', fontweight='bold', fontsize=16)\n",
    "        axes[0, 1].set_ylabel('Timeout Rate (%)', fontsize=14)\n",
    "        axes[0, 1].set_xlabel('Algorithm', fontsize=14)\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].legend(title='Input Type', title_fontsize=12)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Average CPU time (successful runs only)\n",
    "        successful_metrics = self.algorithm_metrics[self.algorithm_metrics['successful_runs'] > 0]\n",
    "        if not successful_metrics.empty:\n",
    "            pivot_cputime = successful_metrics.pivot(index='algorithm', columns='input_type', values='avg_cputime')\n",
    "            pivot_cputime.plot(kind='bar', ax=axes[1, 0], color=['lightgreen', 'orange'])\n",
    "            axes[1, 0].set_title('Average CPU Time (Successful Runs)', fontweight='bold', fontsize=16)\n",
    "            axes[1, 0].set_ylabel('CPU Time (s)', fontsize=14)\n",
    "            axes[1, 0].set_xlabel('Algorithm', fontsize=14)\n",
    "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "            axes[1, 0].legend(title='Input Type', title_fontsize=12)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Average memory usage\n",
    "        if not successful_metrics.empty:\n",
    "            pivot_memory = successful_metrics.pivot(index='algorithm', columns='input_type', values='avg_memory_gb')\n",
    "            pivot_memory.plot(kind='bar', ax=axes[1, 1], color=['lightgreen', 'orange'])\n",
    "            axes[1, 1].set_title('Average Memory Usage (Successful Runs)', fontweight='bold', fontsize=16)\n",
    "            axes[1, 1].set_ylabel('Memory (GB)', fontsize=14)\n",
    "            axes[1, 1].set_xlabel('Algorithm', fontsize=14)\n",
    "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "            axes[1, 1].legend(title='Input Type', title_fontsize=12)\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('algorithm_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Reset font settings to default\n",
    "        plt.rcParams.update(plt.rcParamsDefault)\n",
    "    \n",
    "    def plot_benchmark_category_analysis(self):\n",
    "        \"\"\"Analyze performance by benchmark category\"\"\"\n",
    "        if self.all_data.empty:\n",
    "            self.load_all_data()\n",
    "        \n",
    "        # Set Times New Roman font for this plot as well\n",
    "        plt.rcParams['font.family'] = 'Times New Roman'\n",
    "        plt.rcParams['font.size'] = 12\n",
    "        \n",
    "        # Calculate success rates by category and algorithm\n",
    "        category_stats = self.all_data.groupby(['benchmark_category', 'algorithm', 'input_type']).agg({\n",
    "            'success': 'mean',\n",
    "            'cputime': 'mean',\n",
    "            'memory': 'mean',\n",
    "            'run_name': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        category_stats['success_rate'] = category_stats['success'] * 100\n",
    "        \n",
    "        # Pivot for heatmap\n",
    "        pivot_success = category_stats.pivot_table(\n",
    "            index='benchmark_category', \n",
    "            columns=['algorithm', 'input_type'], \n",
    "            values='success_rate', \n",
    "            aggfunc='mean'\n",
    "        ).fillna(0)\n",
    "        \n",
    "        plt.figure(figsize=(16, 12))\n",
    "        sns.heatmap(pivot_success, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "                   center=50, vmin=0, vmax=100, annot_kws={\"size\": 10})\n",
    "        plt.title('Success Rate by Benchmark Category and Algorithm (%)', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('category_success_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Reset font settings to default\n",
    "        plt.rcParams.update(plt.rcParamsDefault)\n",
    "        \n",
    "        return category_stats\n",
    "    \n",
    "    def rank_algorithms(self):\n",
    "        \"\"\"Rank algorithms based on multiple criteria\"\"\"\n",
    "        if not hasattr(self, 'algorithm_metrics'):\n",
    "            self.calculate_algorithm_metrics()\n",
    "        \n",
    "        # Create scoring system\n",
    "        ranking_data = self.algorithm_metrics.copy()\n",
    "        \n",
    "        # Normalize metrics (higher is better for success, lower is better for time/memory)\n",
    "        ranking_data['score_success'] = ranking_data['success_rate'] / 100\n",
    "        \n",
    "        # Only calculate time/memory scores for algorithms with successful runs\n",
    "        max_cputime = ranking_data['avg_cputime'].max()\n",
    "        max_memory = ranking_data['avg_memory_gb'].max()\n",
    "        \n",
    "        ranking_data['score_time'] = ranking_data.apply(\n",
    "            lambda x: 1 - (x['avg_cputime'] / max_cputime) if max_cputime > 0 and x['successful_runs'] > 0 else 0, \n",
    "            axis=1\n",
    "        )\n",
    "        ranking_data['score_memory'] = ranking_data.apply(\n",
    "            lambda x: 1 - (x['avg_memory_gb'] / max_memory) if max_memory > 0 and x['successful_runs'] > 0 else 0, \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Combined score (weighted)\n",
    "        ranking_data['combined_score'] = (\n",
    "            0.5 * ranking_data['score_success'] +  # Success rate is most important\n",
    "            0.3 * ranking_data['score_time'] +     # Time performance\n",
    "            0.2 * ranking_data['score_memory']     # Memory efficiency\n",
    "        )\n",
    "        \n",
    "        # Rank within each input type\n",
    "        ranking_data['rank'] = ranking_data.groupby('input_type')['combined_score'].rank(ascending=False)\n",
    "        \n",
    "        # Sort by input type and rank\n",
    "        ranking_data = ranking_data.sort_values(['input_type', 'rank'])\n",
    "        \n",
    "        return ranking_data[['algorithm', 'input_type', 'success_rate', 'avg_cputime', \n",
    "                           'avg_memory_gb', 'combined_score', 'rank', 'successful_runs']]\n",
    "    \n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate a comprehensive performance report\"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"COMPREHENSIVE ALGORITHM PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if self.all_data.empty:\n",
    "            self.load_all_data()\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_benchmarks = len(self.all_data)\n",
    "        unique_algorithms = self.all_data['algorithm'].nunique()\n",
    "        unique_benchmarks = self.all_data['benchmark_name'].nunique()\n",
    "        \n",
    "        print(f\"\\nDataset Overview:\")\n",
    "        print(f\"Total benchmark runs: {total_benchmarks}\")\n",
    "        print(f\"Unique algorithms: {unique_algorithms}\")\n",
    "        print(f\"Unique benchmarks: {unique_benchmarks}\")\n",
    "        \n",
    "        # Overall success rates\n",
    "        overall_success = self.all_data['success'].mean() * 100\n",
    "        overall_timeout = self.all_data['timeout'].mean() * 100\n",
    "        overall_oom = self.all_data['oom'].mean() * 100\n",
    "        \n",
    "        print(f\"\\nOverall Statistics:\")\n",
    "        print(f\"Success rate: {overall_success:.1f}%\")\n",
    "        print(f\"Timeout rate: {overall_timeout:.1f}%\")\n",
    "        print(f\"Out-of-memory rate: {overall_oom:.1f}%\")\n",
    "        \n",
    "        # Algorithm rankings\n",
    "        rankings = self.rank_algorithms()\n",
    "        \n",
    "        print(f\"\\nüèÜ ALGORITHM RANKINGS:\")\n",
    "        for input_type in ['BTOR2', 'C']:\n",
    "            print(f\"\\n{input_type} Input Type:\")\n",
    "            input_rankings = rankings[rankings['input_type'] == input_type]\n",
    "            for _, row in input_rankings.iterrows():\n",
    "                print(f\"  {row['rank']:.0f}. {row['algorithm']}: \"\n",
    "                      f\"Success={row['success_rate']:.1f}% ({row['successful_runs']} runs), \"\n",
    "                      f\"CPU Time={row['avg_cputime']:.1f}s, \"\n",
    "                      f\"Score={row['combined_score']:.3f}\")\n",
    "        \n",
    "        # Best overall algorithm\n",
    "        if not rankings.empty:\n",
    "            best_overall = rankings.loc[rankings['combined_score'].idxmax()]\n",
    "            print(f\"\\nüéØ BEST OVERALL ALGORITHM:\")\n",
    "            print(f\"  {best_overall['algorithm']} ({best_overall['input_type']})\")\n",
    "            print(f\"  Success Rate: {best_overall['success_rate']:.1f}%\")\n",
    "            print(f\"  Successful Runs: {best_overall['successful_runs']}\")\n",
    "            print(f\"  Average CPU Time: {best_overall['avg_cputime']:.1f}s\")\n",
    "            print(f\"  Performance Score: {best_overall['combined_score']:.3f}\")\n",
    "        \n",
    "        # Save detailed results\n",
    "        self.all_data.to_csv('all_benchmark_results.csv', index=False)\n",
    "        rankings.to_csv('algorithm_rankings.csv', index=False)\n",
    "        \n",
    "        print(f\"\\nüìä Detailed results saved to:\")\n",
    "        print(f\"  - all_benchmark_results.csv\")\n",
    "        print(f\"  - algorithm_rankings.csv\")\n",
    "        print(f\"  - algorithm_performance_comparison.png\")\n",
    "        print(f\"  - category_success_heatmap.png\")\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run complete analysis pipeline\"\"\"\n",
    "        print(\"Starting comprehensive algorithm analysis...\")\n",
    "        \n",
    "        # Load data\n",
    "        self.load_all_data()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        self.calculate_algorithm_metrics()\n",
    "        \n",
    "        # Generate visualizations\n",
    "        self.plot_success_rates()\n",
    "        self.plot_benchmark_category_analysis()\n",
    "        \n",
    "        # Generate report\n",
    "        self.generate_comprehensive_report()\n",
    "        \n",
    "        print(\"\\n‚úÖ Analysis complete!\")\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Specify the folder containing all XML files\n",
    "    folder_path = \"result_xmls\"  # Current directory, change as needed\n",
    "    \n",
    "    # Initialize comparator\n",
    "    comparator = AlgorithmComparator(folder_path)\n",
    "    \n",
    "    # Run complete analysis\n",
    "    comparator.run_complete_analysis()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
