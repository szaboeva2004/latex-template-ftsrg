\chapter{Evaluation}\label{chapter:evaluation}

This chapter presents a comprehensive evaluation of the \textsc{BTOR2CFA} transformation approach, contrasting it with the traditional translation-based workflow utilizing \textsc{Btor2C}. The experimental methodology and benchmark selection are detailed in~\autoref{sec:experiment_design}. Subsequent sections analyze the structural complexity of the generated models (\autoref{sec:structural}), evaluate the comparative performance of various model checking algorithms (\autoref{sec:algorithm}), investigate the relationship between structural complexity and verification performance (\autoref{sec:structure_and_performance}), and assess the impact of optimization within the transformation pipeline (\autoref{sec:optimization}). A summary of the key findings and a discussion of threats to validity conclude the chapter in \autoref{sec:summary}.

\section{Experiment Design}\label{sec:experiment_design}

This section details the experimental methodology employed to ensure a rigorous comparison. It outlines the hardware and software environment, defines the specific research questions addressing the evaluation goals, and describes the criteria used to curate the benchmark suite.

\subsection{Research Questions}

The evaluation aims to answer the following research questions:

\begin{description}
   \item[RQ1:] How does the structural complexity of the CFAs differ between direct \textsc{Btor2}-to-CFA transformation and the Btor2C translation approach?
   \item[RQ2:] Which model checking algorithms perform best for hardware verification, and how does input type (\textsc{Btor2} vs C) affect performance?
   \item[RQ3:] How does the structural complexity of the CFAs impact verification efficiency?
   \item[RQ4:] How does optimization of BTOR2CFA transformations impact verification efficiency?
\end{description}


\subsection{Benchmark Suite and Methodology}

The Hardware Model Checking Competition (HWMCC)~\cite{hwmcc} provides a comprehensive suite of state-of-the-art benchmark models for evaluating hardware verification tools. In this work, I used a subset of these benchmarks selected according to the following criteria.

First, since my implementation focuses exclusively on safety property verification, only benchmarks containing the \verb|bad| property were considered. Benchmarks featuring liveness-related properties (such as \verb|justice|, \verb|fair|, \verb|output|, and \verb|constraint|) were excluded from the evaluation.

Secondly, as the current implementation of Theta (and consequently BTOR2CFA) does not support overflow predicate or reduce operations, benchmarks utilizing these features were excluded to ensure compatibility.

After applying these filtering criteria, a total of $698$ benchmarks remained. The evaluation utilized both the \textsc{Btor2} and C benchmark subsets provided by HWMCC.

\paragraph{Evaluation Workflows}
To rigorously compare the efficiency of the proposed solution against the state-of-the-art, each circuit was processed through two distinct transformation workflows:
\begin{enumerate}
    \item \textbf{Direct Approach (\textsc{BTOR2CFA}):} The \textsc{Btor2} files were parsed and transformed directly into \textsc{Theta}'s native Control-Flow Automata (CFA) using the frontend developed in this thesis. This approach bypasses all external translation steps.
    \item \textbf{Indirect Approach (C2CFA):} The \textsc{Btor2} files were first translated into C source code using the external \textsc{Btor2C} tool~\cite{btor2c}. These generated C programs were then parsed by \textsc{Theta}'s existing C frontend to produce the final CFA.
\end{enumerate}

The evaluation was conducted in two phases. First, a \textbf{Structural Analysis} was performed where only the transformation to CFA was executed to compare graph metrics (nodes, edges, variables) without running the verification algorithms. Second, a \textbf{Performance Analysis} was conducted where full verification cycles were executed using various model checking algorithms to measure solving time and memory consumption on the generated models.

\paragraph{CFA Analysis}
Several metrics of the Control-Flow Automata (CFAs) generated during the transformation process were systematically compared. This evaluation quantified the number of variables, locations, edges, and operations. To ensure a valid direct comparison, the dataset was filtered to include only the intersection of instances successfully processed by both workflows. Consequently, if a benchmark resulted in a timeout or memory exhaustion in either the C2CFA or BTOR2CFA path, it was excluded from the structural analysis. The remaining $551$ benchmark pairs provided a robust and representative basis for this comparative study. Notably, the BTOR2CFA workflow successfully processed every instance that was solvable by the C implementation.

\paragraph{Performance Analysis}
For the performance evaluation, I compared several verification algorithms supported by \textsc{Theta}. Specifically, I tested Counterexample-Guided Abstraction Refinement (CEGAR)~\cite{cegar} with both explicit-value and predicate abstraction, as well as Bounded Model Checking (BMC)~\cite{bmc}, $k$-Induction~\cite{kind}, and Interpolation-Based Model Checking (IMC)~\cite{imc}.


\subsubsection{Measurement Procedure} 

All experiments were conducted on virtual machines equipped with Intel Xeon (Skylake) processors, featuring between 2 and 32 cores operating at 2.2 GHz, and memory configurations ranging from 16 GB to 128 GB of RAM. The operating system used was Linux 6.8.0-60-generic, and all experiments were performed using \textsc{Theta} version 6.27.12. For SMT solving, I employed \textsc{MathSAT} version 5.6.10~\cite{Cimatti2013MathSAT5}.

To ensure reliable and reproducible performance measurements, I utilized the \textsc{RunExec} tool from the \textsc{BenchExec}\footnote{\url{https://github.com/sosy-lab/benchexec}} suite~\cite{BLW19}, a state-of-the-art benchmarking framework also used in the \textsc{SV-COMP} competition. Each experiment was executed with a CPU time limit of 900 seconds, a memory limit of 15 GB, and restricted to two CPU cores.


% *******************************************************************************************************************

\section{Structural Complexity Analysis}\label{sec:structural}

This section investigates the differences in structural complexity between the CFAs generated by the direct \textsc{Btor2}-to-CFA transformation and those produced by the indirect Btor2C translation approach. The comparative analysis reveals that the direct translation yields a significantly more concise intermediate representation, as illustrated in~\autoref{fig:structural_comp} and~\autoref{tab:metric_ratios_cfa}.

\begin{table}
\centering
\small
\begin{tabular}{@{}lrrrr@{}}
\toprule
 &
\multicolumn{2}{c}{BTOR2CFA} &
\multicolumn{2}{c}{C2CFA} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & Avg & Median & Avg & Median \\
\midrule
Variables     & 1153.93 & 928.0  & 3241.07 & 2561.0 \\
Locations     & 3.00    & 3.0    & 5.00    & 5.0    \\
Edges         & 3.00    & 3.0    & 4.00    & 4.0    \\
Statements    & 1321.20 & 1068.0 & 7543.92 & 6025.0 \\
\bottomrule
\end{tabular}
\caption{Average and median CFA metrics for both workflows.}
\label{tab:metric_ratios_cfa}
\end{table}


\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{figures/structural_comparison.png}
  \caption{Comparison of number og statements showing the scale of complexity expansion between C2CFA and BTOR2CFA transformations.}
  \label{fig:structural_comp}
\end{figure}


\paragraph{Complexity Overhead in C Parsing}~\autoref{fig:structural_comp} demonstrates the relationship between the number of atomic statements and variables across the two formats. The data points cluster well below the $x=y$ diagonal (where $x$ is C), indicating that the high-level C parsing generates a substantially larger number of atomic statements. This is likely due to the introduction of intermediate auxiliary variables and the explicit unrolling of high-level constructs during the translation from C to the CFA intermediate language. Quantitative data presented in~\autoref{tab:metric_ratios_cfa} reveals that the direct transformation \emph{reduces} the variable count to approximately \emph{one-third} of that observed in the indirect approach. In contrast, the indirect transformation exhibits a \emph{six-fold increase} in the number of atomic statements.

As illustrated in~\autoref{fig:structural_comp}, the edge and location counts exhibit a high degree of convergence between the two formats, resulting in negligible ratio differences. This structural similarity is a direct consequence of the optimization pass applied in both workflows -- specifically (sequential) Large-Block Encoding (LBE)~\cite{lbe} -- which effectively compresses sequences of edges into a single edge with several statements. Consequently, edge and location counts lack discriminative power in this context and are therefore excluded from subsequent complexity analyses.

The structural expansion within the C format is a direct consequence of the artifacts introduced during the translation from parallel hardware descriptions to sequential C code. Whereas BTOR2CFA efficiently encodes state utilizing compact bit-vector arrays, the C frontend explicitly instantiates distinct variables for intermediate calculations and memory addressing. Consequently, the BTOR2 format provides a significantly more concise representation for verification tasks within this context.

These structural disparities directly correlate with the performance metrics presented in~\autoref{tab:performance_summary_cfa}. The C2CFA transformation incurs a substantially higher computational overhead in terms of both memory footprint and CPU time. This increased resource consumption provides a clear rationale for the elevated frequency of timeouts and out-of-memory exceptions observed in the indirect approach.

\begin{table}
\centering
\small
\begin{tabular}{@{}lrrrrrrr@{}}
\toprule
Dataset &
\multicolumn{3}{c}{CPU Time (s)} &
\multicolumn{3}{c}{Memory (MB)} &
Success \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & Avg & Median & SD & Avg & Median & SD & (\%) \\
\midrule
C2CFA     & 141.02 & 54.89 & 148.83 & 1878.00 & 984.34 & 2120.07 & 75.43 \\
BTOR2CFA  & 22.42  & 10.46 & 48.93  & 461.02  & 316.43 & 338.16  & 98.71 \\
\bottomrule
\end{tabular}
\caption{Summary of CPU time, memory usage, and success rate for the C2CFA and BTOR2CFA transformations.}
\label{tab:performance_summary_cfa}
\end{table}

\subsection{RQ1: How does the structural complexity differ between direct BTOR2CFA transformation and the Btor2C translation approach?}

The experimental results demonstrate a fundamental structural disparity between the two approaches. The direct \textsc{Btor2}-to-CFA transformation generates a significantly more concise intermediate representation compared to the Btor2C translation. The findings from the structural analysis reveal that the indirect C-based workflow results in approximately a \textbf{three-fold increase in variable count} and a \textbf{six-fold increase in atomic statements}.

This ``structural expansion'' in the C workflow is attributed to the semantic mismatch between the high-level C language and low-level hardware logic. The C frontend introduces numerous auxiliary variables and explicit operations to emulate memory addressing and scope, whereas the direct transformation effectively maps hardware bit-vectors to the CFA's internal representation without such overhead.



% *******************************************************************************************************************


\section{Algorithm Performance Analysis}\label{sec:algorithm}

This section evaluates which model checking algorithms perform best for hardware verification and assesses the impact of input type (\textsc{Btor2} vs C) on their performance. The performance evaluation across different model checking algorithms reveals clear advantages for the direct BTOR2 transformation approach. Tables \ref{tab:btor2_input_summary} and \ref{tab:c_input_summary} summarize the CPU time, memory usage, and success rates for algorithms using BTOR2 and C inputs respectively.

\autoref{fig:performance_btor2_c} presents the quantile plots for CPU time and memory usage across all verified configurations. The results unequivocally demonstrate the superiority of the direct \textsc{Btor2}-to-CFA transformation (represented by dashed lines) over the indirect C-based workflow (solid lines).

\subsection{Analysis of Solved Instances and Resource Consumption}

The quantile plots presented in~\autoref{fig:performance_btor2_c} provide a comprehensive overview of the performance differences between the two transformation workflows. In these plots, the x-axis represents the cumulative number of solved instances, while the y-axis (logarithmic scale) indicates the resource consumption (CPU time and memory) required to solve the $n$-th instance.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{figures/comparison_fig_quantile_c_btor2.png}
  \caption{Quantile plots comparing the CPU time (left) and memory usage (right) of different model checking algorithms. The plots display the cumulative number of solved instances (x-axis) against the resources consumed (logarithmic y-axis) for both BTOR2 and C benchmark sets. Algorithms include CEGAR (Predictive and Explicit), BMC, IMC, and $k$-Induction.}
  \label{fig:performance_btor2_c}
\end{figure}

\subsubsection{Workflow Comparison}
The most prominent observation is the consistent superiority of the direct \textsc{Btor2}-to-CFA transformation (represented by dashed lines) over the indirect translation via C (solid lines). Across all five verifying algorithms (\textsc{CEGAR} with explicit-value and predicative abstraction, BMC, IMC, and $k$-Induction) the direct approach significantly outperforms the indirect approach in terms of total solved instances. The solid curves exhibit a much steeper ascent, indicating that the C-based workflow exhausts resources rapidly even on instances that are computationally trivial for the direct approach. This ``brick wall'' effect confirms the hypothesis that the structural overhead introduced by the C frontend creates a baseline complexity that hinders scalability.

\subsubsection{Algorithmic Effectiveness and Resource Consumption}

Regarding algorithmic performance, Counterexample-Guided Abstraction Refinement with Predicate Abstraction (\texttt{CEGAR\_PRED}) utilizing the direct \textsc{Btor2} input emerges as the unequivocally most robust configuration. As detailed in~\autoref{tab:btor2_input_summary}, it achieves the highest success rate of \textbf{16.62\%}, significantly outperforming the next best algorithm, $k$-Induction (8.74\%). This confirms that predicate abstraction is particularly well-suited for handling the bit-level logic inherent in hardware designs, effectively balancing precision and abstraction.

In terms of resource efficiency, \texttt{CEGAR\_PRED} on \textsc{Btor2} maintains a sustainable footprint. It exhibits a median CPU time of 51.99 s and a median memory usage of 859.59 MB. Although the standard deviation for memory (1859.61 MB) indicates variability across difficult instances, it remains far more efficient than its performance on the C input.

\paragraph{Impact of Input Format on Performance}
Comparing~\autoref{tab:btor2_input_summary} and~\autoref{tab:c_input_summary} reveals the severe performance penalty introduced by the C translation workflow. For the \texttt{CEGAR\_PRED} configuration, switching from \textsc{Btor2} to C causes the success rate to plummet from 16.62\% to \textbf{5.03\%}. Furthermore, the resource consumption increases drastically:
\begin{itemize}
    \item \textbf{Memory Inflation:} The median memory usage for \texttt{CEGAR\_PRED} nearly quadruples, rising from 859.59 MB (BTOR2) to 2990.98 MB (C). Similarly, IMC sees its median memory usage jump from 750.75 MB to 3085.55 MB.
    \item \textbf{Computational Effort:} The median CPU time for \texttt{CEGAR\_PRED} increases from 51.99 s to 129.78 s, indicating that the solver struggles significantly more to reach a conclusion on the C-generated CFAs.
\end{itemize}

\paragraph{The Explicit Value Anomaly}
The \texttt{CEGAR\_EXPL} (Explicit Value Analysis) configuration presents a notable statistical anomaly. As shown in both tables, it reports the lowest average CPU times (e.g., 19.92 s for \textsc{Btor2}) and memory footprints. However, this should not be interpreted as efficiency. With the lowest success rates in the suite (3.58\% for \textsc{Btor2} and a negligible 1.01\% for C), these low resource metrics indicate that the algorithm fails rapidly on non-trivial instances, managing to solve only the simplest benchmarks before quickly exhausting its search capabilities or timing out.

\begin{table}[ht]
\centering
\small
\caption{Summary of CPU time, memory usage, and success rate for algorithms using \textsc{Btor2} as input.}
\label{tab:btor2_input_summary}
\begin{tabular}{@{}lrrrrrrr@{}}
\toprule
Algorithm &
\multicolumn{3}{c}{CPU Time (s)} &
\multicolumn{3}{c}{Memory (MB)} &
Success \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
  & Avg & Median & SD & Avg & Median & SD & (\%) \\
\midrule
BMC         & 182.13 & 61.60 & 252.35 & 696.40  & 649.18 &  384.88    & 7.16 \\
CEGAR\_EXPL & \textbf{19.92}  & \textbf{14.47} & \textbf{13.43}  & \textbf{480.98}  & \textbf{405.40} &  \textbf{295.37}    & 3.58 \\
CEGAR\_PRED & 134.67 & 51.99 & 172.61 & 1717.04 & 859.59 &  1859.61    & \textbf{16.62} \\
IMC         & 159.21 & 52.98 & 216.72 & 1060.91 & 750.75 &  890.43    & 7.31 \\
$k$-Induction & 178.39 & 56.57 & 256.25 & 777.55  & 673.83 &  502.48    & 8.74 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\small
\caption{Summary of CPU time, memory usage, and success rate for algorithms using C as input.}
\label{tab:c_input_summary}
\begin{tabular}{@{}lrrrrrrr@{}}
\toprule
Algorithm &
\multicolumn{3}{c}{CPU Time (s)} &
\multicolumn{3}{c}{Memory (MB)} &
Success \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
  & Avg & Median & SD & Avg & Median & SD & (\%) \\
\midrule
BMC         & \textbf{171.63} & 23.76 & 277.89 & \textbf{1070.70} & 304.83  & \textbf{1892.30}     & 1.01 \\
CEGAR\_EXPL & 232.52 & \textbf{11.53} & 351.92 & 1696.30 & \textbf{297.23}  & 2270.82     & 1.01 \\
CEGAR\_PRED & 270.11 & 129.78& \textbf{259.44} & 4176.83 & 2990.98 & 3436.76     & \textbf{5.03} \\
IMC         & 283.48 & 149.17& 267.46 & 3314.08 & 3085.55 & 3201.35     & 1.87 \\
$k$-Induction & 187.27 & 49.03 & 268.60 & 2506.06 & 2686.79 & 2145.25     & 2.59 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Relationship Between Solver Iterations and Resource Consumption}

Finally, I analyzed the internal behavior of the verification algorithms by examining the relationship between the number of refinement iterations (loops in the algorithm) and the total resource consumption.\autoref{fig:iter_ouput} illustrates the scatter plots for iterations versus CPU time and memory, as well as the distribution of iterations based on the verification result.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{figures/iterations_vs_output.png}
  \caption{Analysis of the relationship between solver iterations and resource consumption. The scatter plots (top and bottom-left) display CPU time and memory usage against iteration counts, with a linear regression line (dashed red) indicating the trend. Data points are distinguished by verification result (False/pink vs. True/gold). The bottom-right panel shows the distribution of iterations required for Safe (True) versus Unsafe (False) outcomes.}
  \label{fig:iter_ouput}
\end{figure}

\paragraph{Decoupling of Iterations and Resources}
While analyzing\autoref{fig:iter_ouput}, a counter-intuitive finding from the scatter plots (top row) is the lack of a strong linear correlation between iteration count and resource usage. The linear regression lines (dashed red) are nearly horizontal for both CPU time and memory. This indicates that the computational cost is not uniformly distributed across iterations. A single, complex refinement step involving a difficult SMT query can consume more resources than dozens of trivial iterations. Consequently, iteration count is a poor predictor of total verification time.

\paragraph{Result-Dependent Complexity}
Furthermore in\autoref{fig:iter_ouput} the box plot analysis (bottom right) reveals a distinct behavioral difference based on the verification outcome. The distribution for \texttt{False} results (unsafe instances) is noticeably wider and has a higher median than that for \texttt{True} (safe) results. This suggests that finding a feasible counterexample often requires the algorithm to unroll the transition system deeper (more iterations) to reach the error state. Conversely, safety proofs often converge faster, likely because a suitable inductive invariant is discovered earlier in the abstraction-refinement loop.

\subsection{RQ2: Which model checking algorithms perform best for hardware verification, and how does input type (\textsc{Btor2} vs C) affect performance?}

\paragraph{Best Algorithm} Counterexample-Guided Abstraction Refinement with Predicate Abstraction (\texttt{CEGAR\_PRED}) utilizing the direct \textsc{Btor2} input is unequivocally the most effective configuration. It achieved the highest success rate ($16.62\%$) and demonstrated the best scalability, solving approximately 115 instances while maintaining moderate resource consumption.

\paragraph{Transformation Impact} Based on my research, the choice of workflow is a decisive factor in verification performance. The C2CFA input induces a severe performance penalty across all tested algorithms.
\begin{description}
    \item [Resource Exhaustion.] The ``brick wall'' effect observed in the quantile plots indicates that C-generated models cause rapid resource exhaustion. For \texttt{CEGAR\_PRED}, switching to C input quadrupled the median memory usage (from $\approx 860$ MB to $\approx 2990$ MB).
    \item [Algorithmic degradation.] Explicit Value Analysis (\texttt{CEGAR\_EXPL}), which struggles generally with hardware logic, fails almost entirely when processing C inputs (1.01\% success rate), confirming that the added structural complexity exacerbates the weaknesses of less robust algorithms.
\end{description}


% ******************************************************************************************************************



\section{Analysis of Structural Complexity and Performance}\label{sec:structure_and_performance}

This section examines the impact of structural complexity on verification efficiency. To better understand the root causes of the performance disparities observed in~\autoref{fig:performance_btor2_c}, I analyzed the Pearson correlation coefficients between structural metrics (Variable Count, Atomic Statements) and performance indicators (CPU Time, Memory). The resulting correlation matrices for the \textsc{Btor2} and C workflows are presented in~\autoref{fig:corr_btor2} and~\autoref{fig:corr_c}, respectively.

\paragraph{Decoupling of Size and Difficulty in Direct Transformation}
The correlation matrices for the \textsc{Btor2} input (\autoref{fig:corr_btor2}) reveal a significant insight: there is a remarkably weak correlation between the structural size of the CFA and the computational resources required to verify it.
For instance, in the \texttt{CEGAR\_PRED} configuration, the correlation between \texttt{Variable Count} and \texttt{CPU Time} is only $0.27$. Even more notably, for Bounded Model Checking (BMC), this correlation drops to $-0.13$.
This suggests that for the direct transformation, the verification complexity is driven primarily by the logical depth and semantic properties of the circuit, rather than the raw size of the state space representation. The solver is effectively able to abstract away the structural components.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{figures/correlation_btor2_algos.png}
  \caption{Feature correlation heatmaps for BTOR2 inputs. The subplots compare the internal dependencies of problem variables and their correlation with runtime and memory usage for BMC, CEGAR (Explicit/Predictive), IMC, and $k$-Induction configurations.}
  \label{fig:corr_btor2}
\end{figure}

\paragraph{Structural Dependency in Indirect Translation}
In stark contrast, the correlation matrices for the C input (\autoref{fig:corr_c}) demonstrate a strong positive correlation between structural metrics and resource consumption across almost all algorithms.
For the BMC algorithm, the correlation between \texttt{Variable Count} and \texttt{CPU Time} rises to $0.82$. Similarly, \texttt{CEGAR\_EXPL} shows a correlation of $0.82$ for the same metric.
This indicates a strong coupling between structure and performance: the ``structural expansion'' introduced by the C frontend (discussed in~\autoref{sec:structural}) creates a linear -- and often prohibitive -- performance penalty. Every additional auxiliary variable or split operation introduced during C parsing directly increases the burden on the solver.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1\textwidth]{figures/correlation_c_algos.png}
  \caption{Feature correlation heatmaps for C inputs. The subplots compare the internal dependencies of problem variables and their correlation with runtime and memory usage for BMC, CEGAR (Explicit/Predictive), IMC, and $k$-Induction configurations.}
  \label{fig:corr_c}
\end{figure}

\paragraph{Impact on Memory Scalability}
The impact of this structural coupling is visible in the memory correlations. In the C workflow, \texttt{Memory} usage correlates strongly with \texttt{Variable Count} (e.g., $0.88$ for BMC). In the \textsc{Btor2} workflow, this correlation is significantly weaker ($0.48$ for BMC). This confirms that the memory exhaustion observed in the quantile plots is largely a function of the inefficient intermediate representation generated by the C frontend, which forces the model checker to track a vastly larger set of state variables than is theoretically necessary.

\subsection{RQ3: How does the structural complexity of the CFAs impact verification efficiency?}

My correlation analysis reveals two distinct behaviors regarding structural complexity and efficiency:
\begin{description}
    \item [Decoupling in Direct Transformation.] For the direct workflow, there is a negligible correlation (e.g., $r \approx -0.13$ for BMC) between the size of the model (variable count) and verification time. This indicates that the direct transformation preserves the logical semantics clearly enough that solvers can abstract away structural size, making performance dependent on \textit{logical} depth rather than \textit{structural} breadth.
    \item [Strong Coupling in Indirect Translation.] Conversely, the C workflow exhibits a strong positive correlation ($r > 0.8$) between variable count and resource usage. The extraneous complexity introduced by the C frontend creates a burden that the solver cannot optimize away, leading to a linear dependency where ``larger'' CFAs invariably result in slower verification and higher memory footprints.
\end{description}



% ******************************************************************************************************************




\section{Impact of Optimization on Verification Efficiency}\label{sec:optimization}

This section investigates the impact of optimization on verification efficiency within the BTOR2CFA pipeline. To address \textbf{RQ4}, I evaluated the performance impact of the optimization passes within the BTOR2CFA transformation pipeline. \autoref{fig:performance_btor2_opt} presents a comparative quantile plot of the \textsc{Btor2}-based workflow with and without optimization enabled.


\begin{figure}
  \centering
  \includegraphics[width=1\textwidth]{figures/comparison_fig_quantile_btor2_opt_no_opt.png}
  \caption{Quantile plots comparing the CPU time (left) and memory usage (right) of different model checking algorithms. The plots display the cumulative number of solved instances (x-axis) against the resources consumed (logarithmic y-axis) for both optimized BTOR2CFA and unoptimized transformations. Algorithms include CEGAR (Predictive and Explicit), BMC, IMC, and $k$-Induction.}
  \label{fig:performance_btor2_opt}
\end{figure}

The results unequivocally demonstrate that optimization is a critical factor for verification viability. The unoptimized configurations (represented by dashed lines) exhibit a severe performance degradation across all algorithms. 
\begin{itemize}
    \item \textbf{CEGAR\_PRED:} The optimized configuration (solid blue line) successfully solves approximately 115 instances. In stark contrast, the unoptimized version (dashed orange line) reaches its limit after solving only roughly 25 instances.
    \item \textbf{BMC and $k$-Induction:} The disparity is even more pronounced for Bounded Model Checking and $k$-Induction. Without optimization, these algorithms fail to solve more than 10 instances, appearing as near-vertical lines on the far left of the plot.
\end{itemize}

This massive performance gap suggests that the raw, direct translation from \textsc{Btor2} to CFA produces a highly redundant control-flow graph. Without the reduction techniques (such as Large-Block Encoding and variable elimination) applied during the optimization phase, the resulting state space is unnecessarily bloated, overwhelming the underlying solvers regardless of the model checking algorithm employed.


\subsection{RQ4: How does optimization of BTOR2CFA transformations impact verification efficiency?}

Optimization is identified not merely as an enhancement, but as a \textbf{prerequisite for viability} in the \textsc{Btor2} verification pipeline. The comparative analysis shows a drastic divergence in performance:
\begin{description}
    \item [Unoptimized.] Without optimization passes (such as Large-Block Encoding), algorithms saturate rapidly, with the best configuration solving fewer than 30 instances. The raw CFA generation produces a state space too redundant for efficient traversal.
    \item [Optimized.] Enabling optimizations allows the solver to handle significantly more complex designs, increasing the solved instance count from $\approx 25$ to $\approx 115$ for \texttt{CEGAR\_PRED}.
\end{description}
Therefore, structural reduction techniques are essential to bridge the gap between the raw translation of hardware circuits and the input requirements of efficient model checking algorithms.
Notably, the unoptimized BTOR2CFA workflow exhibits performance characteristics that are remarkably similar to those of the C2CFA approach.


% ******************************************************************************************************************


\section{Summary of Results}\label{sec:summary}

This work presented BTOR2CFA, a direct transformation from the \textsc{Btor2} hardware description format to Control-Flow Automata (CFA) within the Theta model checking framework. The primary goal was to eliminate the intermediate C translation step introduced by tools like Btor2C, thereby creating a more efficient and semantically precise verification workflow for hardware circuits.

The comparative evaluation yielded clear and significant results. The direct \textsc{Btor2}-to-CFA transformation consistently produced dramatically more compact and less complex models than the translation-based approach. Key findings include:

\begin{description}
  \item[Structural Superiority] \hfill \\ The direct transformation reduced variable counts by approximately 66\% and atomic statements by about 82\% compared to the indirect C-based approach.
  \item[Verification Efficiency] \hfill \\ Across all evaluated algorithms, the direct approach achieved the highest success rate of 16.62\%, significantly outperforming the indirect approach's best 5.03\%. Resource consumption was also drastically lower, with median memory usage reduced from 984 MB to 316 MB.
  \item[Algorithmic Performance] \hfill \\ CEGAR with predicate abstraction (CEGAR\_PRED) emerged as the most effective algorithm for \textsc{Btor2} verification, achieving a 16.62\% success rate on the direct transformation, compared to only 5.03\% on the indirect approach.
  \item[Optimization Impact] \hfill \\ The optimization passes within BTOR2CFA were shown to be essential for verification viability, enabling the solution of over 115 instances compared to fewer than 30 without optimization.
\end{description}

These results demonstrate that eliminating the intermediate C translation step not only preserves hardware semantics more accurately but also enables significant performance gains in both scalability and resource efficiency.

\subsection{Threats to Validity}

\subsubsection{Internal Validity}

Several factors could affect the internal validity of this study:

\begin{itemize}
    \item \textbf{Benchmark Selection:} The evaluation focused on safety property benchmarks from HWMCC, excluding liveness and other property types. This may limit the generalizability of the results to other verification contexts.
    \item \textbf{Tool Versioning:} The experiments were conducted with specific versions of \textsc{Theta} (6.27.12)\footnote{\url{https://github.com/ftsrg/theta}} and \textsc{MathSAT} (5.6.10). Changes in these tools could influence performance outcomes.
    \item \textbf{Configuration Consistency:} While RunExec was used to ensure consistent execution environments, subtle differences in virtual machine allocation or background processes could introduce minor variations in timing and memory measurements.
\end{itemize}

\subsubsection{External Validity}

The external validity of the study is influenced by:

\begin{itemize}
    \item \textbf{Limited Operator Support:} The current implementation does not support all \textsc{Btor2} operators (e.g., reduce operations, overflow predicates, array operations). This restricts the evaluation to a subset of hardware circuits, potentially overlooking performance characteristics in more complex designs.
    \item \textbf{Hardware-Specific Benchmarks:} The study relied exclusively on HWMCC benchmarks, which may not fully represent industrial hardware verification scenarios or emerging circuit designs.
    \item \textbf{Algorithm Scope:} Only a subset of Theta's verification algorithms was evaluated. Other algorithms or configurations might yield different performance trade-offs. It is important to acknowledge that \textsc{Theta} offers extensive configuration capabilities for each algorithm. Consequently, a comprehensive analysis of every possible parameter permutation would result in a combinatorial explosion, rendering such an exhaustive evaluation computationally infeasible.
\end{itemize}